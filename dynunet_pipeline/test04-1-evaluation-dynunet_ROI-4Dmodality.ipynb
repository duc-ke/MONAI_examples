{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a105b425",
   "metadata": {},
   "source": [
    "## Evaluation 4D dynunet pipeline with NeuroI ROI dataset\n",
    "* `train.py`with mode to `val` 을 기반으로함\n",
    "* `commands/val.sh` 을 참고\n",
    "* Brain image + mask image -> 4D modalities\n",
    "* in_channels: 4, out_channels:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cf9fd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "from monai.utils import first\n",
    "from monai.handlers import (\n",
    "    MeanDice,\n",
    "    from_engine,\n",
    ")\n",
    "from monai.inferers import SimpleInferer, SlidingWindowInferer\n",
    "# from monai.utils import set_determinism\n",
    "\n",
    "from create_network import get_network_ke\n",
    "# from evaluator import (\n",
    "#     DynUNetEvaluator,\n",
    "#     DynUNetEvaluator_gpu,\n",
    "#     DynUNetEvaluator_SaveResult,\n",
    "#     DynUNetEvaluator_GPU_SaveResult_PostMapping\n",
    "# )\n",
    "from config import get_config\n",
    "from dataset_roi_4d import get_val_loader   # 4D modality test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d1ce8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = \"/data/kehyeong/project/MONAI_examples/dynunet_pipeline/config_roi_earlystop_toy_220209.yaml\"\n",
    "checkpoint = \"/data/train/running/l/model_roi_try1_220217/models/net_key_metric=0.3331.pt\"\n",
    "val_dataset = \"/work/NeuroI-models/ke-monai/data/roi/dataset_test_roi_toy2.csv\"\n",
    "val_output_dir = \"./runs_eval2\"\n",
    "\n",
    "# tta_val = True    # ?!?!?!  whether to use test time augmentation.\n",
    "tta_val = False\n",
    "\n",
    "multi_gpu_flag = False\n",
    "spacing = [1.0, 1.0, 1.0]\n",
    "deep_supr_num = 3\n",
    "window_mode = \"gaussian\"     # the mode parameter for SlidingWindowInferer.\n",
    "eval_overlap = 0.5\n",
    "amp = False\n",
    "local_rank = 0\n",
    "\n",
    "config = get_config(config)\n",
    "data_dir = config[\"data_dir\"]\n",
    "image_file_path = config[\"image_file_path\"]\n",
    "label_file_path = config[\"label_file_path\"]\n",
    "mask_file_path = config[\"mask_file_path\"]\n",
    "val_batch_size = config[\"val\"][\"batch_size\"] \n",
    "val_num_workers = config[\"val\"][\"num_workers\"]\n",
    "num_classes = config[\"num_classes\"]\n",
    "patch_size = config[\"patch_size\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5ac381",
   "metadata": {},
   "source": [
    "TTA는 말 그대로 Inference(Test) 과정에서 Augmentation 을 적용한 뒤 예측의 확률을 평균(또는 다른 방법)을 통해 도출하는 기법입니다. 모델 학습간에 다양한 Augmentation 을 적용하여 학습하였을시, Inference 과정에서도 유사한 Augmentation 을 적용하여 정확도를 높일 수 있습니다. 또는 이미지에 객체가 너무 작게 위치한 경우 원본 이미지, Crop 이미지를 넣는 등 다양하게 활용이 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18bd7968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{dir}/s_{id}_{aid}_b.nii.gz /data/train/running/l/input_augmented SU0303 00_0\n",
      "{dir}/s_{id}_{aid}_b.nii.gz /data/train/running/l/input_augmented SU0197 00_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|███████████████████████████████████████████████████████████████████████████| 2/2 [00:02<00:00,  1.04s/it]\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(val_output_dir):\n",
    "    os.makedirs(val_output_dir)\n",
    "    \n",
    "if multi_gpu_flag:\n",
    "    dist.init_process_group(backend=\"nccl\", init_method=\"env://\")\n",
    "    device = torch.device(f\"cuda:{local_rank}\")\n",
    "    torch.cuda.set_device(device)\n",
    "else:\n",
    "    device = torch.device(\"cuda:6\")\n",
    "#     device = torch.device(\"cpu\")\n",
    "\n",
    "val_loader = get_val_loader(\n",
    "    data_dir=data_dir,\n",
    "    id_file=val_dataset,\n",
    "    image_file_pattern=image_file_path,\n",
    "    label_file_pattern=label_file_path,\n",
    "    mask_file_pattern=mask_file_path,\n",
    "    batch_size=val_batch_size,\n",
    "    num_workers=val_num_workers,\n",
    "    multi_gpu_flag=multi_gpu_flag\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11d8d65d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['image', 'label', 'mask', 'image_meta_dict', 'label_meta_dict', 'mask_meta_dict', 'image_transforms', 'label_transforms', 'mask_transforms', 'original_shape', 'bbox', 'crop_shape', 'resample_flag', 'anisotrophy_flag'])\n",
      "image shape: torch.Size([1, 2, 154, 154, 152])\n",
      "image dtype: torch.float32\n",
      "label shape: torch.Size([1, 1, 186, 230, 230])\n",
      "label dtype: torch.uint8\n",
      "1번 배치의 유니크한 라벨 리스트: [  0   1   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n",
      "  20  21  23  24  25  26  27  28  29  30  31  32  33  34  36  37  39  40\n",
      "  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58\n",
      "  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76\n",
      "  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94\n",
      "  95  96  97  98  99 100 101 102 103 104 105 106 107 108]\n",
      "1번 배치의 유니크한 라벨 class 수: 104\n"
     ]
    }
   ],
   "source": [
    "test_data = first(val_loader)\n",
    "print(test_data.keys())\n",
    "print(\"image shape:\", test_data['image'].shape)\n",
    "print(\"image dtype:\", test_data['image'].dtype)\n",
    "print(\"label shape:\", test_data['label'].shape)\n",
    "print(\"label dtype:\", test_data['label'].dtype)\n",
    "print(\"1번 배치의 유니크한 라벨 리스트:\", np.unique(test_data['label']))\n",
    "total_labels = np.unique(test_data['label'])\n",
    "print(f'1번 배치의 유니크한 라벨 class 수: {len(total_labels)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6759184",
   "metadata": {},
   "source": [
    "--------------\n",
    "\n",
    "post transform 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d564aafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.transforms import AsDiscrete, Transform\n",
    "post_label = AsDiscrete(to_onehot=num_classes)\n",
    "post_pred = AsDiscrete(argmax=True, to_onehot=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e3ecc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "aaa = post_label(test_data['label'][0])\n",
    "aaa.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b608f41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "aaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457fe182",
   "metadata": {},
   "outputs": [],
   "source": [
    "aaa[:, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8961d975",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['label'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4b0ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['label'][0, :, 80, 80, 80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7ede47",
   "metadata": {},
   "outputs": [],
   "source": [
    "aaa[:, 80, 80, 80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f387c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbb = post_pred(aaa)\n",
    "bbb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa2daab",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbb[:, 80, 80, 80]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0f817a",
   "metadata": {},
   "source": [
    "------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552f828f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25814ac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 109)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "properties = {\n",
    "    'modality': [0,1],\n",
    "    'labels': np.arange(num_classes)\n",
    "}\n",
    "n_class = len(properties[\"labels\"])\n",
    "in_channels = len(properties[\"modality\"])\n",
    "in_channels, n_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58a2caf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretrained checkpoint: /data/train/running/l/model_roi_try1_220217/models/net_key_metric=0.3331.pt loaded\n",
      "Loading nnUNET Done!!!\n",
      "Loading nnUNET to GPU devices Done!!!\n",
      "DynUNet(\n",
      "  (input_block): UnetBasicBlock(\n",
      "    (conv1): Convolution(\n",
      "      (conv): Conv3d(2, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "    )\n",
      "    (conv2): Convolution(\n",
      "      (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "    )\n",
      "    (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (norm1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (norm2): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "  )\n",
      "  (downsamples): ModuleList(\n",
      "    (0): UnetBasicBlock(\n",
      "      (conv1): Convolution(\n",
      "        (conv): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "      )\n",
      "      (conv2): Convolution(\n",
      "        (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      )\n",
      "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "      (norm1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      (norm2): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    )\n",
      "    (1): UnetBasicBlock(\n",
      "      (conv1): Convolution(\n",
      "        (conv): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "      )\n",
      "      (conv2): Convolution(\n",
      "        (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      )\n",
      "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "      (norm1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      (norm2): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    )\n",
      "    (2): UnetBasicBlock(\n",
      "      (conv1): Convolution(\n",
      "        (conv): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "      )\n",
      "      (conv2): Convolution(\n",
      "        (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      )\n",
      "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "      (norm1): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      (norm2): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    )\n",
      "  )\n",
      "  (bottleneck): UnetBasicBlock(\n",
      "    (conv1): Convolution(\n",
      "      (conv): Conv3d(256, 320, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "    )\n",
      "    (conv2): Convolution(\n",
      "      (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "    )\n",
      "    (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (norm1): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (norm2): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "  )\n",
      "  (upsamples): ModuleList(\n",
      "    (0): UnetUpBlock(\n",
      "      (transp_conv): Convolution(\n",
      "        (conv): ConvTranspose3d(320, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
      "      )\n",
      "      (conv_block): UnetBasicBlock(\n",
      "        (conv1): Convolution(\n",
      "          (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        )\n",
      "        (conv2): Convolution(\n",
      "          (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        )\n",
      "        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "        (norm1): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (norm2): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (1): UnetUpBlock(\n",
      "      (transp_conv): Convolution(\n",
      "        (conv): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
      "      )\n",
      "      (conv_block): UnetBasicBlock(\n",
      "        (conv1): Convolution(\n",
      "          (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        )\n",
      "        (conv2): Convolution(\n",
      "          (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        )\n",
      "        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "        (norm1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (norm2): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (2): UnetUpBlock(\n",
      "      (transp_conv): Convolution(\n",
      "        (conv): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
      "      )\n",
      "      (conv_block): UnetBasicBlock(\n",
      "        (conv1): Convolution(\n",
      "          (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        )\n",
      "        (conv2): Convolution(\n",
      "          (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        )\n",
      "        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "        (norm1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (norm2): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (3): UnetUpBlock(\n",
      "      (transp_conv): Convolution(\n",
      "        (conv): ConvTranspose3d(64, 32, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
      "      )\n",
      "      (conv_block): UnetBasicBlock(\n",
      "        (conv1): Convolution(\n",
      "          (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        )\n",
      "        (conv2): Convolution(\n",
      "          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        )\n",
      "        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "        (norm1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (norm2): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (output_block): UnetOutBlock(\n",
      "    (conv): Convolution(\n",
      "      (conv): Conv3d(32, 109, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "    )\n",
      "  )\n",
      "  (deep_supervision_heads): ModuleList(\n",
      "    (0): UnetOutBlock(\n",
      "      (conv): Convolution(\n",
      "        (conv): Conv3d(64, 109, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "      )\n",
      "    )\n",
      "    (1): UnetOutBlock(\n",
      "      (conv): Convolution(\n",
      "        (conv): Conv3d(128, 109, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "      )\n",
      "    )\n",
      "    (2): UnetOutBlock(\n",
      "      (conv): Convolution(\n",
      "        (conv): Conv3d(256, 109, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (skip_layers): DynUNetSkipLayer(\n",
      "    (downsample): UnetBasicBlock(\n",
      "      (conv1): Convolution(\n",
      "        (conv): Conv3d(2, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      )\n",
      "      (conv2): Convolution(\n",
      "        (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      )\n",
      "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "      (norm1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      (norm2): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    )\n",
      "    (next_layer): DynUNetSkipLayer(\n",
      "      (downsample): UnetBasicBlock(\n",
      "        (conv1): Convolution(\n",
      "          (conv): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "        )\n",
      "        (conv2): Convolution(\n",
      "          (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        )\n",
      "        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "        (norm1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (norm2): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "      (next_layer): DynUNetSkipLayer(\n",
      "        (downsample): UnetBasicBlock(\n",
      "          (conv1): Convolution(\n",
      "            (conv): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "          )\n",
      "          (conv2): Convolution(\n",
      "            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "          )\n",
      "          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          (norm1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "          (norm2): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        )\n",
      "        (next_layer): DynUNetSkipLayer(\n",
      "          (downsample): UnetBasicBlock(\n",
      "            (conv1): Convolution(\n",
      "              (conv): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "            )\n",
      "            (conv2): Convolution(\n",
      "              (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "            )\n",
      "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "            (norm1): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "            (norm2): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "          )\n",
      "          (next_layer): UnetBasicBlock(\n",
      "            (conv1): Convolution(\n",
      "              (conv): Conv3d(256, 320, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "            )\n",
      "            (conv2): Convolution(\n",
      "              (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "            )\n",
      "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "            (norm1): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "            (norm2): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "          )\n",
      "          (upsample): UnetUpBlock(\n",
      "            (transp_conv): Convolution(\n",
      "              (conv): ConvTranspose3d(320, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
      "            )\n",
      "            (conv_block): UnetBasicBlock(\n",
      "              (conv1): Convolution(\n",
      "                (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "              )\n",
      "              (conv2): Convolution(\n",
      "                (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "              )\n",
      "              (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "              (norm1): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "              (norm2): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "            )\n",
      "          )\n",
      "          (super_head): UnetOutBlock(\n",
      "            (conv): Convolution(\n",
      "              (conv): Conv3d(256, 109, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (upsample): UnetUpBlock(\n",
      "          (transp_conv): Convolution(\n",
      "            (conv): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
      "          )\n",
      "          (conv_block): UnetBasicBlock(\n",
      "            (conv1): Convolution(\n",
      "              (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "            )\n",
      "            (conv2): Convolution(\n",
      "              (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "            )\n",
      "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "            (norm1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "            (norm2): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "          )\n",
      "        )\n",
      "        (super_head): UnetOutBlock(\n",
      "          (conv): Convolution(\n",
      "            (conv): Conv3d(128, 109, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (upsample): UnetUpBlock(\n",
      "        (transp_conv): Convolution(\n",
      "          (conv): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
      "        )\n",
      "        (conv_block): UnetBasicBlock(\n",
      "          (conv1): Convolution(\n",
      "            (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "          )\n",
      "          (conv2): Convolution(\n",
      "            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "          )\n",
      "          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          (norm1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "          (norm2): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        )\n",
      "      )\n",
      "      (super_head): UnetOutBlock(\n",
      "        (conv): Convolution(\n",
      "          (conv): Conv3d(64, 109, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (upsample): UnetUpBlock(\n",
      "      (transp_conv): Convolution(\n",
      "        (conv): ConvTranspose3d(64, 32, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
      "      )\n",
      "      (conv_block): UnetBasicBlock(\n",
      "        (conv1): Convolution(\n",
      "          (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        )\n",
      "        (conv2): Convolution(\n",
      "          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        )\n",
      "        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "        (norm1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (norm2): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# produce the network\n",
    "net = get_network_ke(properties, patch_size, spacing, deep_supr_num, \n",
    "                     val_output_dir, checkpoint)    # val_output_dir은 삭제 필요. 안씀\n",
    "print('Loading nnUNET Done!!!')\n",
    "net = net.to(device)\n",
    "print('Loading nnUNET to GPU devices Done!!!')\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f256c681",
   "metadata": {},
   "outputs": [],
   "source": [
    "if multi_gpu_flag:\n",
    "    net = DistributedDataParallel(module=net, device_ids=[device])\n",
    "\n",
    "num_classes = len(properties[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "352432b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=6)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4524af9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 1. 기본형\n",
    "# net.eval()\n",
    "# evaluator = DynUNetEvaluator(\n",
    "#     device=device,\n",
    "#     val_data_loader=val_loader,\n",
    "#     network=net,\n",
    "#     num_classes=num_classes,\n",
    "#     inferer=SlidingWindowInferer(\n",
    "#         roi_size=patch_size,\n",
    "#         sw_batch_size=val_batch_size,\n",
    "#         overlap=eval_overlap,\n",
    "#         mode=window_mode,\n",
    "#     ),\n",
    "#     postprocessing=None,      # 이걸 바꿔줘야할듯...\n",
    "#     key_val_metric={\n",
    "#         \"val_mean_dice\": MeanDice(\n",
    "#             include_background=False,\n",
    "#             output_transform=from_engine([\"pred\", \"label\"]),\n",
    "#         )\n",
    "#     },\n",
    "#     additional_metrics=None,\n",
    "#     amp=amp,\n",
    "#     tta_val=tta_val,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e6648c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 2. GPU inference 형\n",
    "# net.eval()\n",
    "# evaluator = DynUNetEvaluator_gpu(\n",
    "#     device=device,\n",
    "#     val_data_loader=val_loader,\n",
    "#     network=net,\n",
    "#     num_classes=num_classes,\n",
    "#     inferer=SlidingWindowInferer(\n",
    "#         roi_size=patch_size,\n",
    "#         sw_batch_size=val_batch_size,\n",
    "#         overlap=eval_overlap,\n",
    "#         mode=window_mode,\n",
    "#     ),\n",
    "#     postprocessing=None,      # 이걸 바꿔줘야할듯...\n",
    "#     key_val_metric={\n",
    "#         \"val_mean_dice\": MeanDice(\n",
    "#             include_background=False,\n",
    "#             output_transform=from_engine([\"pred\", \"label\"]),\n",
    "#         )\n",
    "#     },\n",
    "#     additional_metrics=None,\n",
    "#     amp=amp,\n",
    "#     tta_val=tta_val,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027b8862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 3. GPU ver. + inference image 저장 + tta False\n",
    "\n",
    "# net.eval()\n",
    "# evaluator = DynUNetEvaluator_SaveResult(\n",
    "#     device=device,\n",
    "#     val_data_loader=val_loader,\n",
    "#     network=net,\n",
    "#     output_dir=val_output_dir,\n",
    "#     num_classes=num_classes,\n",
    "#     inferer=SlidingWindowInferer(\n",
    "#         roi_size=patch_size,\n",
    "#         sw_batch_size=val_batch_size,\n",
    "#         overlap=eval_overlap,\n",
    "#         mode=window_mode,\n",
    "#     ),\n",
    "#     postprocessing=None,      # 이걸 바꿔줘야할듯...\n",
    "#     key_val_metric={\n",
    "#         \"val_mean_dice\": MeanDice(\n",
    "#             include_background=False,\n",
    "#             output_transform=from_engine([\"pred\", \"label\"]),\n",
    "#         )\n",
    "#     },\n",
    "#     additional_metrics=None,\n",
    "#     amp=amp,\n",
    "#     tta_val=tta_val,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010943c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 4. GPU ver. + inference image 저장형 + tta False + posttransform (mapping index)\n",
    "# from monai.transforms import (\n",
    "#     LoadImaged,\n",
    "#     AddChanneld,\n",
    "#     MapLabelValued,\n",
    "#     Compose\n",
    "# )\n",
    "\n",
    "# orig_label_classes, target_label_classes = (\n",
    "#     np.array([   0,    2,    3,    4,    5,    7,    8,   10,   11,   12,   13,\n",
    "#          14,   15,   16,   17,   18,   24,   26,   28,   30,   31,   41,\n",
    "#          42,   43,   44,   46,   47,   49,   50,   51,   52,   53,   54,\n",
    "#          58,   60,   62,   63,   77,   80,   85,  251,  252,  253,  254,\n",
    "#         255, 1000, 1002, 1003, 1005, 1006, 1007, 1008, 1009, 1010, 1011,\n",
    "#        1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022,\n",
    "#        1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1034, 1035,\n",
    "#        2000, 2002, 2003, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012,\n",
    "#        2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023,\n",
    "#        2024, 2025, 2026, 2027, 2028, 2029, 2030, 2031, 2034, 2035]),\n",
    "#     np.array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
    "#         13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
    "#         26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
    "#         39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
    "#         52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
    "#         65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
    "#         78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
    "#         91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
    "#        104, 105, 106, 107, 108])\n",
    "# )\n",
    "\n",
    "# post_trans = MapLabelValued(\n",
    "#     keys=[\"pred\"], \n",
    "#     orig_labels=target_label_classes, \n",
    "#     target_labels=orig_label_classes\n",
    "# )\n",
    "\n",
    "\n",
    "# net.eval()\n",
    "# evaluator = DynUNetEvaluator_SaveResult(\n",
    "#     device=device,\n",
    "#     val_data_loader=val_loader,\n",
    "#     network=net,\n",
    "#     output_dir=val_output_dir,\n",
    "#     num_classes=num_classes,\n",
    "#     inferer=SlidingWindowInferer(\n",
    "#         roi_size=patch_size,\n",
    "#         sw_batch_size=val_batch_size,\n",
    "#         overlap=eval_overlap,\n",
    "#         mode=window_mode,\n",
    "#     ),\n",
    "#     postprocessing=post_trans,      # 이걸 바꿔줘야할듯...\n",
    "#     key_val_metric={\n",
    "#         \"val_mean_dice\": MeanDice(\n",
    "#             include_background=False,\n",
    "#             output_transform=from_engine([\"pred\", \"label\"]),\n",
    "#         )\n",
    "#     },\n",
    "#     additional_metrics=None,\n",
    "#     amp=amp,\n",
    "#     tta_val=tta_val,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f823b6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from typing import Any, Callable, Dict, Optional, Sequence, Tuple, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from ignite.engine import Engine\n",
    "from ignite.metrics import Metric\n",
    "from monai.data import decollate_batch\n",
    "from monai.data.nifti_writer import write_nifti\n",
    "from monai.engines import SupervisedEvaluator\n",
    "from monai.engines.utils import CommonKeys as Keys\n",
    "from monai.engines.utils import IterationEvents, default_prepare_batch\n",
    "from monai.inferers import Inferer\n",
    "from monai.networks.utils import eval_mode\n",
    "from monai.transforms import AsDiscrete, Transform\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transforms import recovery_prediction\n",
    "\n",
    "from monai.transforms import Compose, AddChannel, MapLabelValue, CastToType\n",
    "\n",
    "class DynUNetEvaluator_GPU_SaveResult_PostMapping(SupervisedEvaluator):\n",
    "    \"\"\"\n",
    "    넣어준 모든 이미지들의 DiceMean 계산 뿐 아니라 inference 결과 이미지도 저장하도록 변경\n",
    "\n",
    "    Args:\n",
    "        device: an object representing the device on which to run.\n",
    "        val_data_loader: Ignite engine use data_loader to run, must be\n",
    "            torch.DataLoader.\n",
    "        network: use the network to run model forward.\n",
    "        num_classes: the number of classes (output channels) for the task.\n",
    "        epoch_length: number of iterations for one epoch, default to\n",
    "            `len(val_data_loader)`.\n",
    "        non_blocking: if True and this copy is between CPU and GPU, the copy may occur asynchronously\n",
    "            with respect to the host. For other cases, this argument has no effect.\n",
    "        prepare_batch: function to parse image and label for current iteration.\n",
    "        iteration_update: the callable function for every iteration, expect to accept `engine`\n",
    "            and `batchdata` as input parameters. if not provided, use `self._iteration()` instead.\n",
    "        inferer: inference method that execute model forward on input data, like: SlidingWindow, etc.\n",
    "        postprocessing: execute additional transformation for the model output data.\n",
    "            Typically, several Tensor based transforms composed by `Compose`.\n",
    "        key_val_metric: compute metric when every iteration completed, and save average value to\n",
    "            engine.state.metrics when epoch completed. key_val_metric is the main metric to compare and save the\n",
    "            checkpoint into files.\n",
    "        additional_metrics: more Ignite metrics that also attach to Ignite Engine.\n",
    "        val_handlers: every handler is a set of Ignite Event-Handlers, must have `attach` function, like:\n",
    "            CheckpointHandler, StatsHandler, SegmentationSaver, etc.\n",
    "        amp: whether to enable auto-mixed-precision evaluation, default is False.\n",
    "        tta_val: whether to do the 8 flips (8 = 2 ** 3, where 3 represents the three dimensions)\n",
    "            test time augmentation, default is False.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        device: torch.device,\n",
    "        val_data_loader: DataLoader,\n",
    "        network: torch.nn.Module,\n",
    "        output_dir: str,                                      # infer specific\n",
    "        num_classes: Union[str, int],\n",
    "        epoch_length: Optional[int] = None,\n",
    "        non_blocking: bool = False,\n",
    "        prepare_batch: Callable = default_prepare_batch,\n",
    "        iteration_update: Optional[Callable] = None,\n",
    "        inferer: Optional[Inferer] = None,\n",
    "        postprocessing: Optional[Transform] = None,\n",
    "        key_val_metric: Optional[Dict[str, Metric]] = None,\n",
    "        additional_metrics: Optional[Dict[str, Metric]] = None,\n",
    "        val_handlers: Optional[Sequence] = None,\n",
    "        amp: bool = False,\n",
    "        tta_val: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__(\n",
    "            device=device,\n",
    "            val_data_loader=val_data_loader,\n",
    "            network=network,\n",
    "            epoch_length=epoch_length,\n",
    "            non_blocking=non_blocking,\n",
    "            prepare_batch=prepare_batch,\n",
    "            iteration_update=iteration_update,\n",
    "            inferer=inferer,\n",
    "            postprocessing=postprocessing,\n",
    "            key_val_metric=key_val_metric,\n",
    "            additional_metrics=additional_metrics,\n",
    "            val_handlers=val_handlers,\n",
    "            amp=amp,\n",
    "        )\n",
    "\n",
    "        if not isinstance(num_classes, int):\n",
    "            num_classes = int(num_classes)\n",
    "        self.output_dir = output_dir               # infer specific\n",
    "        self.num_classes = num_classes\n",
    "        self.post_pred = AsDiscrete(argmax=True, to_onehot=num_classes)\n",
    "        self.post_label = AsDiscrete(to_onehot=num_classes)              # eval specific\n",
    "        self.tta_val = tta_val\n",
    "        \n",
    "        # orig_label_classes, target_label_classes = (\n",
    "        #     np.array([   0,    2,    3,    4,    5,    7,    8,   10,   11,   12,   13,\n",
    "        #         14,   15,   16,   17,   18,   24,   26,   28,   30,   31,   41,\n",
    "        #         42,   43,   44,   46,   47,   49,   50,   51,   52,   53,   54,\n",
    "        #         58,   60,   62,   63,   77,   80,   85,  251,  252,  253,  254,\n",
    "        #         255, 1000, 1002, 1003, 1005, 1006, 1007, 1008, 1009, 1010, 1011,\n",
    "        #     1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022,\n",
    "        #     1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1034, 1035,\n",
    "        #     2000, 2002, 2003, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012,\n",
    "        #     2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023,\n",
    "        #     2024, 2025, 2026, 2027, 2028, 2029, 2030, 2031, 2034, 2035], dtype=np.float64),\n",
    "        #     np.array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
    "        #         13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
    "        #         26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
    "        #         39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
    "        #         52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
    "        #         65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
    "        #         78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
    "        #         91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
    "        #     104, 105, 106, 107, 108], dtype=np.float64)\n",
    "        # )\n",
    "        # self.orig_label_classes = orig_label_classes\n",
    "        # self.target_label_classes = target_label_classes\n",
    "        # self.post_trans = Compose([\n",
    "        #     AddChannel(),\n",
    "        #     MapLabelValue(    # 0~108로 잡혀있는 label을 original index로 변경\n",
    "        #                   orig_labels=self.target_label_classes, \n",
    "        #                   target_labels=self.orig_label_classes,\n",
    "        #                   dtype=np.uint8\n",
    "        #                   )\n",
    "        #     ])\n",
    "        # self.post_trans = MapLabelValue(    # 0~108로 잡혀있는 label을 original index로 변경\n",
    "        #                                 orig_labels=self.target_label_classes, \n",
    "        #                                 target_labels=self.orig_label_classes\n",
    "        #                   )\n",
    "\n",
    "    def _iteration(\n",
    "        self, engine: Engine, batchdata: Dict[str, Any]\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        callback function for the Supervised Evaluation processing logic of 1 iteration in Ignite Engine.\n",
    "        Return below items in a dictionary:\n",
    "            - IMAGE: image Tensor data for model input, already moved to device.\n",
    "            - LABEL: label Tensor data corresponding to the image, already moved to device.\n",
    "            - PRED: prediction result of model.\n",
    "\n",
    "        Args:\n",
    "            engine: Ignite Engine, it can be a trainer, validator or evaluator.\n",
    "            batchdata: input data for this iteration, usually can be dictionary or tuple of Tensor data.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: When ``batchdata`` is None.\n",
    "\n",
    "        \"\"\"\n",
    "        if batchdata is None:\n",
    "            raise ValueError(\"Must provide batch data for current iteration.\")\n",
    "        batch = self.prepare_batch(batchdata, engine.state.device, engine.non_blocking)\n",
    "        if len(batch) == 2:\n",
    "            inputs, targets = batch\n",
    "            args: Tuple = ()\n",
    "            kwargs: Dict = {}\n",
    "        else:\n",
    "            inputs, targets, args, kwargs = batch\n",
    "\n",
    "        targets = targets.cpu()      # device로 바꿔보자\n",
    "        # print('CPU 비활성화!!!!!!!!!! ')\n",
    "\n",
    "        def _compute_pred():\n",
    "            ct = 1.0\n",
    "            # pred = self.inferer(inputs, self.network, *args, **kwargs).cpu()    # device로 바꿔보자 (지우면댐)\n",
    "            pred = self.inferer(inputs, self.network, *args, **kwargs)    # device로 바꿔보자 (지우면댐)\n",
    "            pred = nn.functional.softmax(pred, dim=1)\n",
    "            if not self.tta_val:\n",
    "                return pred\n",
    "            else:\n",
    "                for dims in [[2], [3], [4], (2, 3), (2, 4), (3, 4), (2, 3, 4)]:\n",
    "                    flip_inputs = torch.flip(inputs, dims=dims)\n",
    "                    flip_pred = torch.flip(\n",
    "                        # self.inferer(flip_inputs, self.network).cpu(), dims=dims    # device로 바꿔보자\n",
    "                        self.inferer(flip_inputs, self.network), dims=dims    # device로 바꿔보자\n",
    "                    )\n",
    "                    flip_pred = nn.functional.softmax(flip_pred, dim=1)\n",
    "                    del flip_inputs\n",
    "                    pred += flip_pred\n",
    "                    del flip_pred\n",
    "                    ct += 1\n",
    "                return pred / ct\n",
    "\n",
    "        # execute forward computation\n",
    "        with eval_mode(self.network):\n",
    "            if self.amp:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    predictions = _compute_pred()\n",
    "            else:\n",
    "                predictions = _compute_pred()\n",
    "\n",
    "        inputs = inputs.cpu()    #  # device로 바꿔보자\n",
    "\n",
    "        predictions = self.post_pred(decollate_batch(predictions)[0])\n",
    "        targets = self.post_label(decollate_batch(targets)[0])                # eval specific\n",
    "\n",
    "        affine = batchdata[\"image_meta_dict\"][\"affine\"].numpy()[0]            # infer specific\n",
    "        resample_flag = batchdata[\"resample_flag\"]\n",
    "        anisotrophy_flag = batchdata[\"anisotrophy_flag\"]\n",
    "        crop_shape = batchdata[\"crop_shape\"][0].tolist()\n",
    "        original_shape = batchdata[\"original_shape\"][0].tolist()\n",
    "        if resample_flag:\n",
    "            # convert the prediction back to the original (after cropped) shape\n",
    "            predictions = recovery_prediction(\n",
    "                predictions.numpy(), [self.num_classes, *crop_shape], anisotrophy_flag\n",
    "            )\n",
    "            predictions = torch.tensor(predictions)\n",
    "\n",
    "        ## 이미지 저장\n",
    "        predictions_wirte = predictions.cpu()\n",
    "        print(type(predictions_wirte))\n",
    "        print(predictions_wirte.shape)\n",
    "        predictions_wirte = np.argmax(predictions_wirte, axis=0)\n",
    "        print(predictions_wirte.shape)\n",
    "        predictions_wirte_org = np.zeros([*original_shape])\n",
    "        \n",
    "        # put iteration outputs into engine.state\n",
    "        engine.state.output = {Keys.IMAGE: inputs, Keys.LABEL: targets.unsqueeze(0)}\n",
    "        engine.state.output[Keys.PRED] = torch.zeros([1, self.num_classes, *original_shape])\n",
    "        # pad the prediction back to the original shape\n",
    "        box_start, box_end = batchdata[\"bbox\"][0]\n",
    "        h_start, w_start, d_start = box_start\n",
    "        h_end, w_end, d_end = box_end\n",
    "\n",
    "        engine.state.output[Keys.PRED][\n",
    "            0, :, h_start:h_end, w_start:w_end, d_start:d_end\n",
    "        ] = predictions\n",
    "        del predictions\n",
    "\n",
    "        \n",
    "        ## 이미지 저장\n",
    "        # predictions_wirte = self.post_trans(predictions_wirte)\n",
    "        orig_label_classes, target_label_classes = (\n",
    "            np.array([   0,    2,    3,    4,    5,    7,    8,   10,   11,   12,   13,\n",
    "                14,   15,   16,   17,   18,   24,   26,   28,   30,   31,   41,\n",
    "                42,   43,   44,   46,   47,   49,   50,   51,   52,   53,   54,\n",
    "                58,   60,   62,   63,   77,   80,   85,  251,  252,  253,  254,\n",
    "                255, 1000, 1002, 1003, 1005, 1006, 1007, 1008, 1009, 1010, 1011,\n",
    "            1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022,\n",
    "            1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1034, 1035,\n",
    "            2000, 2002, 2003, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012,\n",
    "            2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023,\n",
    "            2024, 2025, 2026, 2027, 2028, 2029, 2030, 2031, 2034, 2035]),\n",
    "            np.array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
    "                13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
    "                26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
    "                39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
    "                52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
    "                65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
    "                78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
    "                91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
    "            104, 105, 106, 107, 108])\n",
    "        )\n",
    "        post_trans = MapLabelValue(    # 0~108로 잡혀있는 label을 original index로 변경\n",
    "            orig_labels=target_label_classes, \n",
    "            target_labels=orig_label_classes,\n",
    "#             dtype=np.uint8\n",
    "        )\n",
    "#         post_trans = Compose([\n",
    "#             MapLabelValue(    # 0~108로 잡혀있는 label을 original index로 변경\n",
    "#             orig_labels=target_label_classes, \n",
    "#             target_labels=orig_label_classes,\n",
    "# #             dtype=np.uint8\n",
    "#             ),\n",
    "#             CastToType(dtype=np.int64)\n",
    "            \n",
    "#         ])\n",
    "               \n",
    "        \n",
    "        predictions_wirte_org[h_start:h_end, w_start:w_end, d_start:d_end] = predictions_wirte\n",
    "        del predictions_wirte\n",
    "        print('변형전 dtype', predictions_wirte_org.dtype)\n",
    "        print(np.unique(predictions_wirte_org))\n",
    "        # predictions_wirte_org = self.post_trans(predictions_wirte_org)   # 원래대로 pred index 복구\n",
    "        # predictions_wirte_org = predictions_wirte_org.squeeze()\n",
    "        predictions_wirte_org = post_trans(predictions_wirte_org)\n",
    "        print('변형후 dtype', predictions_wirte_org.dtype)\n",
    "        print(np.unique(predictions_wirte_org))\n",
    "        \n",
    "        filename = batchdata[\"image_meta_dict\"][\"filename_or_obj\"][0].split(\"/\")[-1]\n",
    "        print(\n",
    "            \"save {} with shape: {}\".format(\n",
    "                filename, predictions_wirte_org.shape\n",
    "            )\n",
    "        )\n",
    "        write_nifti(\n",
    "            data=predictions_wirte_org,\n",
    "            file_name=os.path.join(self.output_dir, filename),\n",
    "            affine=affine,\n",
    "            resample=False,\n",
    "            output_dtype=np.uint32,\n",
    "        )\n",
    "        print(f'MRI img:{ filename } eval done .................')\n",
    "        engine.fire_event(IterationEvents.FORWARD_COMPLETED)\n",
    "        engine.fire_event(IterationEvents.MODEL_COMPLETED)\n",
    "\n",
    "        return engine.state.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ad0da258",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from typing import Any, Callable, Dict, Optional, Sequence, Tuple, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from ignite.engine import Engine\n",
    "from ignite.metrics import Metric\n",
    "from monai.data import decollate_batch\n",
    "from monai.data.nifti_writer import write_nifti\n",
    "from monai.engines import SupervisedEvaluator\n",
    "from monai.engines.utils import CommonKeys as Keys\n",
    "from monai.engines.utils import IterationEvents, default_prepare_batch\n",
    "from monai.inferers import Inferer\n",
    "from monai.networks.utils import eval_mode\n",
    "from monai.transforms import AsDiscrete, Transform\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transforms import recovery_prediction\n",
    "\n",
    "from monai.transforms import Compose, AddChannel, MapLabelValue, CastToType\n",
    "\n",
    "class DynUNetEvaluator_GPU_SaveResult_PostMapping2(SupervisedEvaluator):\n",
    "    \"\"\"\n",
    "    넣어준 모든 이미지들의 DiceMean 계산 뿐 아니라 inference 결과 이미지도 저장하도록 변경\n",
    "\n",
    "    Args:\n",
    "        device: an object representing the device on which to run.\n",
    "        val_data_loader: Ignite engine use data_loader to run, must be\n",
    "            torch.DataLoader.\n",
    "        network: use the network to run model forward.\n",
    "        num_classes: the number of classes (output channels) for the task.\n",
    "        epoch_length: number of iterations for one epoch, default to\n",
    "            `len(val_data_loader)`.\n",
    "        non_blocking: if True and this copy is between CPU and GPU, the copy may occur asynchronously\n",
    "            with respect to the host. For other cases, this argument has no effect.\n",
    "        prepare_batch: function to parse image and label for current iteration.\n",
    "        iteration_update: the callable function for every iteration, expect to accept `engine`\n",
    "            and `batchdata` as input parameters. if not provided, use `self._iteration()` instead.\n",
    "        inferer: inference method that execute model forward on input data, like: SlidingWindow, etc.\n",
    "        postprocessing: execute additional transformation for the model output data.\n",
    "            Typically, several Tensor based transforms composed by `Compose`.\n",
    "        key_val_metric: compute metric when every iteration completed, and save average value to\n",
    "            engine.state.metrics when epoch completed. key_val_metric is the main metric to compare and save the\n",
    "            checkpoint into files.\n",
    "        additional_metrics: more Ignite metrics that also attach to Ignite Engine.\n",
    "        val_handlers: every handler is a set of Ignite Event-Handlers, must have `attach` function, like:\n",
    "            CheckpointHandler, StatsHandler, SegmentationSaver, etc.\n",
    "        amp: whether to enable auto-mixed-precision evaluation, default is False.\n",
    "        tta_val: whether to do the 8 flips (8 = 2 ** 3, where 3 represents the three dimensions)\n",
    "            test time augmentation, default is False.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        device: torch.device,\n",
    "        val_data_loader: DataLoader,\n",
    "        network: torch.nn.Module,\n",
    "        output_dir: str,                                      # infer specific\n",
    "        num_classes: Union[str, int],\n",
    "        epoch_length: Optional[int] = None,\n",
    "        non_blocking: bool = False,\n",
    "        prepare_batch: Callable = default_prepare_batch,\n",
    "        iteration_update: Optional[Callable] = None,\n",
    "        inferer: Optional[Inferer] = None,\n",
    "        postprocessing: Optional[Transform] = None,\n",
    "        key_val_metric: Optional[Dict[str, Metric]] = None,\n",
    "        additional_metrics: Optional[Dict[str, Metric]] = None,\n",
    "        val_handlers: Optional[Sequence] = None,\n",
    "        amp: bool = False,\n",
    "        tta_val: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__(\n",
    "            device=device,\n",
    "            val_data_loader=val_data_loader,\n",
    "            network=network,\n",
    "            epoch_length=epoch_length,\n",
    "            non_blocking=non_blocking,\n",
    "            prepare_batch=prepare_batch,\n",
    "            iteration_update=iteration_update,\n",
    "            inferer=inferer,\n",
    "            postprocessing=postprocessing,\n",
    "            key_val_metric=key_val_metric,\n",
    "            additional_metrics=additional_metrics,\n",
    "            val_handlers=val_handlers,\n",
    "            amp=amp,\n",
    "        )\n",
    "\n",
    "        if not isinstance(num_classes, int):\n",
    "            num_classes = int(num_classes)\n",
    "        self.output_dir = output_dir               # infer specific\n",
    "        self.num_classes = num_classes\n",
    "        self.post_pred = AsDiscrete(argmax=True, to_onehot=num_classes)\n",
    "        self.post_label = AsDiscrete(to_onehot=num_classes)              # eval specific\n",
    "        self.tta_val = tta_val\n",
    "        \n",
    "        orig_label_classes, target_label_classes = (\n",
    "            np.array([   0,    2,    3,    4,    5,    7,    8,   10,   11,   12,   13,\n",
    "                14,   15,   16,   17,   18,   24,   26,   28,   30,   31,   41,\n",
    "                42,   43,   44,   46,   47,   49,   50,   51,   52,   53,   54,\n",
    "                58,   60,   62,   63,   77,   80,   85,  251,  252,  253,  254,\n",
    "                255, 1000, 1002, 1003, 1005, 1006, 1007, 1008, 1009, 1010, 1011,\n",
    "            1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022,\n",
    "            1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1034, 1035,\n",
    "            2000, 2002, 2003, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012,\n",
    "            2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023,\n",
    "            2024, 2025, 2026, 2027, 2028, 2029, 2030, 2031, 2034, 2035], dtype=np.float64),\n",
    "            np.array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
    "                13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
    "                26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
    "                39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
    "                52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
    "                65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
    "                78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
    "                91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
    "            104, 105, 106, 107, 108], dtype=np.float64)\n",
    "        )\n",
    "        self.orig_label_classes = orig_label_classes\n",
    "        self.target_label_classes = target_label_classes\n",
    "        # self.post_trans = Compose([\n",
    "        #     AddChannel(),\n",
    "        #     MapLabelValue(    # 0~108로 잡혀있는 label을 original index로 변경\n",
    "        #                   orig_labels=self.target_label_classes, \n",
    "        #                   target_labels=self.orig_label_classes,\n",
    "        #                   dtype=np.uint8\n",
    "        #                   )\n",
    "        #     ])\n",
    "        self.post_trans = MapLabelValue(    # 0~108로 잡혀있는 label을 original index로 변경\n",
    "                                        orig_labels=self.target_label_classes, \n",
    "                                        target_labels=self.orig_label_classes\n",
    "                          )\n",
    "\n",
    "    def _iteration(\n",
    "        self, engine: Engine, batchdata: Dict[str, Any]\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        callback function for the Supervised Evaluation processing logic of 1 iteration in Ignite Engine.\n",
    "        Return below items in a dictionary:\n",
    "            - IMAGE: image Tensor data for model input, already moved to device.\n",
    "            - LABEL: label Tensor data corresponding to the image, already moved to device.\n",
    "            - PRED: prediction result of model.\n",
    "\n",
    "        Args:\n",
    "            engine: Ignite Engine, it can be a trainer, validator or evaluator.\n",
    "            batchdata: input data for this iteration, usually can be dictionary or tuple of Tensor data.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: When ``batchdata`` is None.\n",
    "\n",
    "        \"\"\"\n",
    "        if batchdata is None:\n",
    "            raise ValueError(\"Must provide batch data for current iteration.\")\n",
    "        batch = self.prepare_batch(batchdata, engine.state.device, engine.non_blocking)\n",
    "        if len(batch) == 2:\n",
    "            inputs, targets = batch\n",
    "            args: Tuple = ()\n",
    "            kwargs: Dict = {}\n",
    "        else:\n",
    "            inputs, targets, args, kwargs = batch\n",
    "\n",
    "        targets = targets.cpu()      # device로 바꿔보자\n",
    "        # print('CPU 비활성화!!!!!!!!!! ')\n",
    "\n",
    "        def _compute_pred():\n",
    "            ct = 1.0\n",
    "            # pred = self.inferer(inputs, self.network, *args, **kwargs).cpu()    # device로 바꿔보자 (지우면댐)\n",
    "            pred = self.inferer(inputs, self.network, *args, **kwargs)    # device로 바꿔보자 (지우면댐)\n",
    "            pred = nn.functional.softmax(pred, dim=1)\n",
    "            if not self.tta_val:\n",
    "                return pred\n",
    "            else:\n",
    "                for dims in [[2], [3], [4], (2, 3), (2, 4), (3, 4), (2, 3, 4)]:\n",
    "                    flip_inputs = torch.flip(inputs, dims=dims)\n",
    "                    flip_pred = torch.flip(\n",
    "                        # self.inferer(flip_inputs, self.network).cpu(), dims=dims    # device로 바꿔보자\n",
    "                        self.inferer(flip_inputs, self.network), dims=dims    # device로 바꿔보자\n",
    "                    )\n",
    "                    flip_pred = nn.functional.softmax(flip_pred, dim=1)\n",
    "                    del flip_inputs\n",
    "                    pred += flip_pred\n",
    "                    del flip_pred\n",
    "                    ct += 1\n",
    "                return pred / ct\n",
    "\n",
    "        # execute forward computation\n",
    "        with eval_mode(self.network):\n",
    "            if self.amp:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    predictions = _compute_pred()\n",
    "            else:\n",
    "                predictions = _compute_pred()\n",
    "\n",
    "        inputs = inputs.cpu()    #  # device로 바꿔보자\n",
    "\n",
    "        predictions = self.post_pred(decollate_batch(predictions)[0])\n",
    "        targets = self.post_label(decollate_batch(targets)[0])                # eval specific\n",
    "\n",
    "        affine = batchdata[\"image_meta_dict\"][\"affine\"].numpy()[0]            # infer specific\n",
    "        resample_flag = batchdata[\"resample_flag\"]\n",
    "        anisotrophy_flag = batchdata[\"anisotrophy_flag\"]\n",
    "        crop_shape = batchdata[\"crop_shape\"][0].tolist()\n",
    "        original_shape = batchdata[\"original_shape\"][0].tolist()\n",
    "        if resample_flag:\n",
    "            # convert the prediction back to the original (after cropped) shape\n",
    "            predictions = recovery_prediction(\n",
    "                predictions.numpy(), [self.num_classes, *crop_shape], anisotrophy_flag\n",
    "            )\n",
    "            predictions = torch.tensor(predictions)\n",
    "\n",
    "        ## 이미지 저장\n",
    "        predictions_wirte = predictions.cpu()\n",
    "        print(type(predictions_wirte))\n",
    "        print(predictions_wirte.shape)\n",
    "        predictions_wirte = np.argmax(predictions_wirte, axis=0)\n",
    "        print(predictions_wirte.shape)\n",
    "        predictions_wirte_org = np.zeros([*original_shape])\n",
    "        \n",
    "        # put iteration outputs into engine.state\n",
    "        engine.state.output = {Keys.IMAGE: inputs, Keys.LABEL: targets.unsqueeze(0)}\n",
    "        engine.state.output[Keys.PRED] = torch.zeros([1, self.num_classes, *original_shape])\n",
    "        # pad the prediction back to the original shape\n",
    "        box_start, box_end = batchdata[\"bbox\"][0]\n",
    "        h_start, w_start, d_start = box_start\n",
    "        h_end, w_end, d_end = box_end\n",
    "\n",
    "        engine.state.output[Keys.PRED][\n",
    "            0, :, h_start:h_end, w_start:w_end, d_start:d_end\n",
    "        ] = predictions\n",
    "        del predictions\n",
    "\n",
    "        \n",
    "        ## 이미지 저장\n",
    "        # predictions_wirte = self.post_trans(predictions_wirte)\n",
    "#         orig_label_classes, target_label_classes = (\n",
    "#             np.array([   0,    2,    3,    4,    5,    7,    8,   10,   11,   12,   13,\n",
    "#                 14,   15,   16,   17,   18,   24,   26,   28,   30,   31,   41,\n",
    "#                 42,   43,   44,   46,   47,   49,   50,   51,   52,   53,   54,\n",
    "#                 58,   60,   62,   63,   77,   80,   85,  251,  252,  253,  254,\n",
    "#                 255, 1000, 1002, 1003, 1005, 1006, 1007, 1008, 1009, 1010, 1011,\n",
    "#             1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022,\n",
    "#             1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1034, 1035,\n",
    "#             2000, 2002, 2003, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012,\n",
    "#             2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023,\n",
    "#             2024, 2025, 2026, 2027, 2028, 2029, 2030, 2031, 2034, 2035]),\n",
    "#             np.array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
    "#                 13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
    "#                 26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
    "#                 39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
    "#                 52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
    "#                 65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
    "#                 78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
    "#                 91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
    "#             104, 105, 106, 107, 108])\n",
    "#         )\n",
    "#         post_trans = MapLabelValue(    # 0~108로 잡혀있는 label을 original index로 변경\n",
    "#             orig_labels=target_label_classes, \n",
    "#             target_labels=orig_label_classes,\n",
    "#         )\n",
    "#         post_trans = Compose([\n",
    "#             MapLabelValue(    # 0~108로 잡혀있는 label을 original index로 변경\n",
    "#             orig_labels=target_label_classes, \n",
    "#             target_labels=orig_label_classes,\n",
    "# #             dtype=np.uint8\n",
    "#             ),\n",
    "#             CastToType(dtype=np.int64)\n",
    "            \n",
    "#         ])\n",
    "               \n",
    "        \n",
    "        predictions_wirte_org[h_start:h_end, w_start:w_end, d_start:d_end] = predictions_wirte\n",
    "        del predictions_wirte\n",
    "        print('변형전 dtype', predictions_wirte_org.dtype)\n",
    "        print(np.unique(predictions_wirte_org))\n",
    "        predictions_wirte_org = self.post_trans(predictions_wirte_org)   # 원래대로 pred index 복구\n",
    "        # predictions_wirte_org = predictions_wirte_org.squeeze()\n",
    "#         predictions_wirte_org = post_trans(predictions_wirte_org)\n",
    "        print('변형후 dtype', predictions_wirte_org.dtype)\n",
    "        print(np.unique(predictions_wirte_org))\n",
    "        \n",
    "        filename = batchdata[\"image_meta_dict\"][\"filename_or_obj\"][0].split(\"/\")[-1]\n",
    "        print(\n",
    "            \"save {} with shape: {}\".format(\n",
    "                filename, predictions_wirte_org.shape\n",
    "            )\n",
    "        )\n",
    "        write_nifti(\n",
    "            data=predictions_wirte_org,\n",
    "            file_name=os.path.join(self.output_dir, filename),\n",
    "            affine=affine,\n",
    "            resample=False,\n",
    "            output_dtype=np.uint32,\n",
    "        )\n",
    "        print(f'MRI img:{ filename } eval done .................')\n",
    "        engine.fire_event(IterationEvents.FORWARD_COMPLETED)\n",
    "        engine.fire_event(IterationEvents.MODEL_COMPLETED)\n",
    "\n",
    "        return engine.state.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "17f4e27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5. GPU ver. + inference image 저장형 + tta False + posttransform 2 (mapping index)\n",
    "# evaluator class 내부에서 transform\n",
    "net.eval()\n",
    "evaluator = DynUNetEvaluator_GPU_SaveResult_PostMapping2(\n",
    "    device=device,\n",
    "    val_data_loader=val_loader,\n",
    "    network=net,\n",
    "    output_dir=val_output_dir,\n",
    "    num_classes=num_classes,\n",
    "    inferer=SlidingWindowInferer(\n",
    "        roi_size=patch_size,\n",
    "        sw_batch_size=val_batch_size,\n",
    "        overlap=eval_overlap,\n",
    "        mode=window_mode,\n",
    "    ),\n",
    "    postprocessing=None,\n",
    "    key_val_metric={\n",
    "        \"val_mean_dice\": MeanDice(\n",
    "            include_background=False,\n",
    "            output_transform=from_engine([\"pred\", \"label\"]),\n",
    "        )\n",
    "    },\n",
    "    additional_metrics=None,\n",
    "    amp=amp,\n",
    "    tta_val=tta_val,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "50a9ceac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([109, 154, 154, 152])\n",
      "torch.Size([154, 154, 152])\n",
      "변형전 dtype float64\n",
      "[  0.   1.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.\n",
      "  15.  16.  17.  18.  20.  21.  23.  24.  25.  26.  27.  28.  29.  30.\n",
      "  31.  32.  33.  34.  36.  37.  39.  40.  41.  42.  43.  44.  45.  46.\n",
      "  47.  48.  49.  50.  51.  52.  53.  54.  55.  56.  57.  58.  59.  60.\n",
      "  61.  62.  63.  64.  65.  66.  67.  68.  69.  70.  71.  72.  73.  74.\n",
      "  75.  76.  77.  78.  79.  80.  81.  82.  83.  84.  85.  86.  87.  88.\n",
      "  89.  90.  91.  92.  93.  94.  95.  96.  97.  98.  99. 100. 101. 102.\n",
      " 103. 104. 105. 106. 107. 108.]\n",
      "변형후 dtype float32\n",
      "[0.000e+00 2.000e+00 4.000e+00 5.000e+00 7.000e+00 8.000e+00 1.000e+01\n",
      " 1.100e+01 1.200e+01 1.300e+01 1.400e+01 1.500e+01 1.600e+01 1.700e+01\n",
      " 1.800e+01 2.400e+01 2.600e+01 2.800e+01 3.100e+01 4.100e+01 4.300e+01\n",
      " 4.400e+01 4.600e+01 4.700e+01 4.900e+01 5.000e+01 5.100e+01 5.200e+01\n",
      " 5.300e+01 5.400e+01 5.800e+01 6.000e+01 6.300e+01 7.700e+01 8.500e+01\n",
      " 2.510e+02 2.520e+02 2.530e+02 2.540e+02 2.550e+02 1.000e+03 1.002e+03\n",
      " 1.003e+03 1.005e+03 1.006e+03 1.007e+03 1.008e+03 1.009e+03 1.010e+03\n",
      " 1.011e+03 1.012e+03 1.013e+03 1.014e+03 1.015e+03 1.016e+03 1.017e+03\n",
      " 1.018e+03 1.019e+03 1.020e+03 1.021e+03 1.022e+03 1.023e+03 1.024e+03\n",
      " 1.025e+03 1.026e+03 1.027e+03 1.028e+03 1.029e+03 1.030e+03 1.031e+03\n",
      " 1.034e+03 1.035e+03 2.000e+03 2.002e+03 2.003e+03 2.005e+03 2.006e+03\n",
      " 2.007e+03 2.008e+03 2.009e+03 2.010e+03 2.011e+03 2.012e+03 2.013e+03\n",
      " 2.014e+03 2.015e+03 2.016e+03 2.017e+03 2.018e+03 2.019e+03 2.020e+03\n",
      " 2.021e+03 2.022e+03 2.023e+03 2.024e+03 2.025e+03 2.026e+03 2.027e+03\n",
      " 2.028e+03 2.029e+03 2.030e+03 2.031e+03 2.034e+03 2.035e+03]\n",
      "save s_SU0303_00_0_b.nii.gz with shape: (186, 230, 230)\n",
      "MRI img:s_SU0303_00_0_b.nii.gz eval done .................\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([109, 141, 146, 133])\n",
      "torch.Size([141, 146, 133])\n",
      "변형전 dtype float64\n",
      "[  0.   1.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.\n",
      "  15.  16.  17.  18.  20.  21.  23.  24.  25.  26.  27.  28.  29.  30.\n",
      "  31.  32.  33.  34.  36.  37.  39.  40.  41.  42.  43.  44.  45.  46.\n",
      "  47.  48.  49.  50.  51.  52.  53.  54.  55.  56.  57.  58.  59.  60.\n",
      "  61.  62.  63.  64.  65.  66.  67.  68.  69.  70.  71.  72.  73.  74.\n",
      "  75.  76.  77.  78.  79.  80.  81.  82.  83.  84.  85.  86.  87.  88.\n",
      "  89.  90.  91.  92.  93.  94.  95.  96.  97.  98.  99. 100. 101. 102.\n",
      " 103. 104. 105. 106. 107. 108.]\n",
      "변형후 dtype float32\n",
      "[0.000e+00 2.000e+00 4.000e+00 5.000e+00 7.000e+00 8.000e+00 1.000e+01\n",
      " 1.100e+01 1.200e+01 1.300e+01 1.400e+01 1.500e+01 1.600e+01 1.700e+01\n",
      " 1.800e+01 2.400e+01 2.600e+01 2.800e+01 3.100e+01 4.100e+01 4.300e+01\n",
      " 4.400e+01 4.600e+01 4.700e+01 4.900e+01 5.000e+01 5.100e+01 5.200e+01\n",
      " 5.300e+01 5.400e+01 5.800e+01 6.000e+01 6.300e+01 7.700e+01 8.500e+01\n",
      " 2.510e+02 2.520e+02 2.530e+02 2.540e+02 2.550e+02 1.000e+03 1.002e+03\n",
      " 1.003e+03 1.005e+03 1.006e+03 1.007e+03 1.008e+03 1.009e+03 1.010e+03\n",
      " 1.011e+03 1.012e+03 1.013e+03 1.014e+03 1.015e+03 1.016e+03 1.017e+03\n",
      " 1.018e+03 1.019e+03 1.020e+03 1.021e+03 1.022e+03 1.023e+03 1.024e+03\n",
      " 1.025e+03 1.026e+03 1.027e+03 1.028e+03 1.029e+03 1.030e+03 1.031e+03\n",
      " 1.034e+03 1.035e+03 2.000e+03 2.002e+03 2.003e+03 2.005e+03 2.006e+03\n",
      " 2.007e+03 2.008e+03 2.009e+03 2.010e+03 2.011e+03 2.012e+03 2.013e+03\n",
      " 2.014e+03 2.015e+03 2.016e+03 2.017e+03 2.018e+03 2.019e+03 2.020e+03\n",
      " 2.021e+03 2.022e+03 2.023e+03 2.024e+03 2.025e+03 2.026e+03 2.027e+03\n",
      " 2.028e+03 2.029e+03 2.030e+03 2.031e+03 2.034e+03 2.035e+03]\n",
      "save s_SU0197_00_0_b.nii.gz with shape: (186, 230, 230)\n",
      "MRI img:s_SU0197_00_0_b.nii.gz eval done .................\n",
      "time : 33.53588509559631\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "evaluator.run()\n",
    "print(\"time :\", time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e6bc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "a[93]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401e7457",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1503f32c",
   "metadata": {},
   "source": [
    "이미지를 로드하여 mapping transform 테스트를 진행해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "1b1647af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.transforms import (\n",
    "    LoadImaged,\n",
    "    AddChanneld,\n",
    "    MapLabelValued,\n",
    "    Compose\n",
    ")\n",
    "from monai.data import (\n",
    "    CacheDataset,\n",
    "    DataLoader,\n",
    "    partition_dataset,\n",
    ")\n",
    "\n",
    "orig_label_classes, target_label_classes = (\n",
    "    np.array([   0,    2,    3,    4,    5,    7,    8,   10,   11,   12,   13,\n",
    "         14,   15,   16,   17,   18,   24,   26,   28,   30,   31,   41,\n",
    "         42,   43,   44,   46,   47,   49,   50,   51,   52,   53,   54,\n",
    "         58,   60,   62,   63,   77,   80,   85,  251,  252,  253,  254,\n",
    "        255, 1000, 1002, 1003, 1005, 1006, 1007, 1008, 1009, 1010, 1011,\n",
    "       1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022,\n",
    "       1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1034, 1035,\n",
    "       2000, 2002, 2003, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012,\n",
    "       2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023,\n",
    "       2024, 2025, 2026, 2027, 2028, 2029, 2030, 2031, 2034, 2035]),\n",
    "    np.array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
    "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
    "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
    "        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
    "        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
    "        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
    "        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
    "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
    "       104, 105, 106, 107, 108])\n",
    ")\n",
    "\n",
    "def get_test_transform():\n",
    "    keys = [\"label\", \"pred\"]\n",
    "    transforms = [\n",
    "        LoadImaged(keys=keys),\n",
    "#         AddChanneld(keys=keys),\n",
    "#         MapLabelValued(\n",
    "#             keys=[\"pred\"], \n",
    "#             orig_labels=target_label_classes, \n",
    "#             target_labels=orig_label_classes\n",
    "#         ),\n",
    "#         Orientationd(keys=keys, axcodes=\"RAS\"),\n",
    "#         ConcatItemsd(keys=[\"image\", \"mask\"], name=\"image\"),\n",
    "#         PreprocessAnisotropic(\n",
    "#             keys=[\"image\"],\n",
    "#             clip_values=clip_values,\n",
    "#             pixdim=spacing,\n",
    "#             normalize_values=normalize_values,\n",
    "#             model_mode=\"test\",\n",
    "#         ),\n",
    "#         CastToTyped(keys=[\"image\"], dtype=(np.float32)),\n",
    "#         EnsureTyped(keys=[\"image\"]),\n",
    "    ]\n",
    "    return Compose(transforms)\n",
    "transform = get_test_transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f7a6434a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "# original_file = os.path.join(data_dir, \"s_SU0197_00_0_b.nii.gz\")\n",
    "# shutil.copy(original_file, './')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "5e7b6a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|███████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.78it/s]\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "label_file = os.path.join(data_dir, \"s_SU0197_00_0_l.nii.gz\")\n",
    "pred_file = \"runs_eval2/s_SU0197_00_0_b.nii.gz\"\n",
    "data.append({'label': label_file, 'pred': pred_file})\n",
    "\n",
    "dataset = CacheDataset(\n",
    "    data=data,\n",
    "    transform=transform,\n",
    "    num_workers=8,\n",
    "    cache_rate=1.0,\n",
    ")\n",
    "\n",
    "data_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    drop_last=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "698a5d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = first(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "4327d813",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['label', 'pred', 'label_meta_dict', 'pred_meta_dict'])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "2484c374",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9.99280810e-01, -2.96830740e-02,  2.35963799e-02,\n",
       "        -9.13983536e+01],\n",
       "       [ 2.94676349e-02,  9.99521315e-01,  9.42613184e-03,\n",
       "        -1.21794128e+02],\n",
       "       [-2.38648821e-02, -8.72402266e-03,  9.99677122e-01,\n",
       "        -9.46715775e+01],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "affine = test_data[\"pred_meta_dict\"][\"affine\"].numpy()[0]\n",
    "affine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "50195c74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 186, 230, 230]), torch.Size([1, 186, 230, 230]))"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data['label'].shape, test_data['pred'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "bec60fbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.float32, torch.float32)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data['pred'].dtype, test_data['label'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "0cb82558",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(104,\n",
       " array([0.000e+00, 2.000e+00, 4.000e+00, 5.000e+00, 7.000e+00, 8.000e+00,\n",
       "        1.000e+01, 1.100e+01, 1.200e+01, 1.300e+01, 1.400e+01, 1.500e+01,\n",
       "        1.600e+01, 1.700e+01, 1.800e+01, 2.400e+01, 2.600e+01, 2.800e+01,\n",
       "        3.100e+01, 4.100e+01, 4.300e+01, 4.400e+01, 4.600e+01, 4.700e+01,\n",
       "        4.900e+01, 5.000e+01, 5.100e+01, 5.200e+01, 5.300e+01, 5.400e+01,\n",
       "        5.800e+01, 6.000e+01, 6.300e+01, 7.700e+01, 8.500e+01, 2.510e+02,\n",
       "        2.520e+02, 2.530e+02, 2.540e+02, 2.550e+02, 1.000e+03, 1.002e+03,\n",
       "        1.003e+03, 1.005e+03, 1.006e+03, 1.007e+03, 1.008e+03, 1.009e+03,\n",
       "        1.010e+03, 1.011e+03, 1.012e+03, 1.013e+03, 1.014e+03, 1.015e+03,\n",
       "        1.016e+03, 1.017e+03, 1.018e+03, 1.019e+03, 1.020e+03, 1.021e+03,\n",
       "        1.022e+03, 1.023e+03, 1.024e+03, 1.025e+03, 1.026e+03, 1.027e+03,\n",
       "        1.028e+03, 1.029e+03, 1.030e+03, 1.031e+03, 1.034e+03, 1.035e+03,\n",
       "        2.000e+03, 2.002e+03, 2.003e+03, 2.005e+03, 2.006e+03, 2.007e+03,\n",
       "        2.008e+03, 2.009e+03, 2.010e+03, 2.011e+03, 2.012e+03, 2.013e+03,\n",
       "        2.014e+03, 2.015e+03, 2.016e+03, 2.017e+03, 2.018e+03, 2.019e+03,\n",
       "        2.020e+03, 2.021e+03, 2.022e+03, 2.023e+03, 2.024e+03, 2.025e+03,\n",
       "        2.026e+03, 2.027e+03, 2.028e+03, 2.029e+03, 2.030e+03, 2.031e+03,\n",
       "        2.034e+03, 2.035e+03], dtype=float32))"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(test_data['pred'])), np.unique(test_data['pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "cbc47810",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.000e+00, 2.000e+00, 4.000e+00, 5.000e+00, 7.000e+00, 8.000e+00,\n",
       "       1.000e+01, 1.100e+01, 1.200e+01, 1.300e+01, 1.400e+01, 1.500e+01,\n",
       "       1.600e+01, 1.700e+01, 1.800e+01, 2.400e+01, 2.600e+01, 2.800e+01,\n",
       "       3.100e+01, 4.100e+01, 4.300e+01, 4.400e+01, 4.600e+01, 4.700e+01,\n",
       "       4.900e+01, 5.000e+01, 5.100e+01, 5.200e+01, 5.300e+01, 5.400e+01,\n",
       "       5.800e+01, 6.000e+01, 6.300e+01, 7.700e+01, 8.500e+01, 2.510e+02,\n",
       "       2.520e+02, 2.530e+02, 2.540e+02, 2.550e+02, 1.000e+03, 1.002e+03,\n",
       "       1.003e+03, 1.005e+03, 1.006e+03, 1.007e+03, 1.008e+03, 1.009e+03,\n",
       "       1.010e+03, 1.011e+03, 1.012e+03, 1.013e+03, 1.014e+03, 1.015e+03,\n",
       "       1.016e+03, 1.017e+03, 1.018e+03, 1.019e+03, 1.020e+03, 1.021e+03,\n",
       "       1.022e+03, 1.023e+03, 1.024e+03, 1.025e+03, 1.026e+03, 1.027e+03,\n",
       "       1.028e+03, 1.029e+03, 1.030e+03, 1.031e+03, 1.034e+03, 1.035e+03,\n",
       "       2.000e+03, 2.002e+03, 2.003e+03, 2.005e+03, 2.006e+03, 2.007e+03,\n",
       "       2.008e+03, 2.009e+03, 2.010e+03, 2.011e+03, 2.012e+03, 2.013e+03,\n",
       "       2.014e+03, 2.015e+03, 2.016e+03, 2.017e+03, 2.018e+03, 2.019e+03,\n",
       "       2.020e+03, 2.021e+03, 2.022e+03, 2.023e+03, 2.024e+03, 2.025e+03,\n",
       "       2.026e+03, 2.027e+03, 2.028e+03, 2.029e+03, 2.030e+03, 2.031e+03,\n",
       "       2.034e+03, 2.035e+03], dtype=float32)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(test_data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "3623580e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([125, 125, 125, ..., 150, 150, 150]),\n",
       " array([132, 133, 133, ..., 134, 134, 134]),\n",
       " array([132, 131, 132, ..., 135, 136, 137]))"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = np.where(test_data['pred'] == 2018)   # 2018, 93\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "9f256e75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2018.)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 12\n",
    "test_data['pred'][result[0][idx], result[1][idx], result[2][idx], result[3][idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "b55ab9e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2018.)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data['label'][result[0][idx], result[1][idx], result[2][idx], result[3][idx]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb520ed",
   "metadata": {},
   "source": [
    "오케 맵핑이 잘되고 있음을 확인하였음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f171626a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e9b1f22c",
   "metadata": {},
   "source": [
    "------------------\n",
    "저장된 이미지로부터 post-transform mapping 하여 결과 확인해본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfd4653",
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.transforms import MapLabelValue\n",
    "orig_label_classes, target_label_classes = (\n",
    "    np.array([   0,    2,    3,    4,    5,    7,    8,   10,   11,   12,   13,\n",
    "        14,   15,   16,   17,   18,   24,   26,   28,   30,   31,   41,\n",
    "        42,   43,   44,   46,   47,   49,   50,   51,   52,   53,   54,\n",
    "        58,   60,   62,   63,   77,   80,   85,  251,  252,  253,  254,\n",
    "        255, 1000, 1002, 1003, 1005, 1006, 1007, 1008, 1009, 1010, 1011,\n",
    "    1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022,\n",
    "    1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1034, 1035,\n",
    "    2000, 2002, 2003, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012,\n",
    "    2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023,\n",
    "    2024, 2025, 2026, 2027, 2028, 2029, 2030, 2031, 2034, 2035]),\n",
    "    np.array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
    "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
    "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
    "        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
    "        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
    "        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
    "        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
    "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
    "    104, 105, 106, 107, 108])\n",
    ")\n",
    "post_trans = MapLabelValue(    # 0~108로 잡혀있는 label을 original index로 변경\n",
    "    orig_labels=target_label_classes, \n",
    "    target_labels=orig_label_classes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f15843f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['pred'].shape, test_data['label'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ce2246",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "aaa = post_trans(test_data['pred'])\n",
    "aaa.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b537b087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aaa = test_data['pred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d499240",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = np.where(aaa == 2018)   # 41\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ff527a",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 226\n",
    "aaa[result[0][idx], result[1][idx], result[2][idx], result[3][idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f0a114",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['label'][result[0][idx], result[1][idx], result[2][idx], result[3][idx]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e846231e",
   "metadata": {},
   "source": [
    "post transform 한 결과를 이미지로 저장한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040761a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "aaa.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2135698b",
   "metadata": {},
   "outputs": [],
   "source": [
    "aaa.squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b5051c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.data.nifti_writer import write_nifti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf26d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_nifti(\n",
    "        data=aaa.squeeze(),\n",
    "        file_name='aa2.nii.gz',\n",
    "        affine=affine,\n",
    "        resample=False,\n",
    "        output_dtype=np.uint8,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8eca068",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614e963b",
   "metadata": {},
   "outputs": [],
   "source": [
    "aaa.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd15e82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94ed36e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350a9fef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8417bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.transforms import (\n",
    "    LoadImaged,\n",
    "    AddChanneld,\n",
    "    MapLabelValued,\n",
    "    Compose\n",
    ")\n",
    "from monai.data import (\n",
    "    CacheDataset,\n",
    "    DataLoader,\n",
    "    partition_dataset,\n",
    ")\n",
    "\n",
    "orig_label_classes, target_label_classes = (\n",
    "    np.array([   0,    2,    3,    4,    5,    7,    8,   10,   11,   12,   13,\n",
    "         14,   15,   16,   17,   18,   24,   26,   28,   30,   31,   41,\n",
    "         42,   43,   44,   46,   47,   49,   50,   51,   52,   53,   54,\n",
    "         58,   60,   62,   63,   77,   80,   85,  251,  252,  253,  254,\n",
    "        255, 1000, 1002, 1003, 1005, 1006, 1007, 1008, 1009, 1010, 1011,\n",
    "       1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022,\n",
    "       1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1034, 1035,\n",
    "       2000, 2002, 2003, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012,\n",
    "       2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023,\n",
    "       2024, 2025, 2026, 2027, 2028, 2029, 2030, 2031, 2034, 2035]),\n",
    "    np.array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
    "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
    "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
    "        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
    "        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
    "        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
    "        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
    "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
    "       104, 105, 106, 107, 108])\n",
    ")\n",
    "\n",
    "def get_test_transform():\n",
    "    keys = [\"label\", \"pred\"]\n",
    "    transforms = [\n",
    "        LoadImaged(keys=keys),\n",
    "#         AddChanneld(keys=keys),\n",
    "#         MapLabelValued(\n",
    "#             keys=[\"pred\"], \n",
    "#             orig_labels=target_label_classes, \n",
    "#             target_labels=orig_label_classes\n",
    "#         ),\n",
    "#         Orientationd(keys=keys, axcodes=\"RAS\"),\n",
    "#         ConcatItemsd(keys=[\"image\", \"mask\"], name=\"image\"),\n",
    "#         PreprocessAnisotropic(\n",
    "#             keys=[\"image\"],\n",
    "#             clip_values=clip_values,\n",
    "#             pixdim=spacing,\n",
    "#             normalize_values=normalize_values,\n",
    "#             model_mode=\"test\",\n",
    "#         ),\n",
    "#         CastToTyped(keys=[\"image\"], dtype=(np.float32)),\n",
    "#         EnsureTyped(keys=[\"image\"]),\n",
    "    ]\n",
    "    return Compose(transforms)\n",
    "transform = get_test_transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932c7c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "label_file = os.path.join(data_dir, \"s_SU0197_00_0_l.nii.gz\")\n",
    "pred_file = \"aa2.nii.gz\"\n",
    "data.append({'label': label_file, 'pred': pred_file})\n",
    "\n",
    "dataset = CacheDataset(\n",
    "    data=data,\n",
    "    transform=transform,\n",
    "    num_workers=8,\n",
    "    cache_rate=1.0,\n",
    ")\n",
    "\n",
    "data_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    drop_last=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff375cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = first(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42259bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2257dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = np.where(test_data['pred'] == 2018)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b7ef13",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 12\n",
    "test_data['pred'][result[0][idx], result[1][idx], result[2][idx], result[3][idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04a126d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['label'][result[0][idx], result[1][idx], result[2][idx], result[3][idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3131f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d37d2731",
   "metadata": {},
   "source": [
    "## finding 정리\n",
    "evaluator에 MeanDice가 저장되는 개념.\n",
    " * evaluator.state.metrics : 전체 ROI MeanDice 평균\n",
    " * evaluator.state.metric_details[\"val_mean_dice\"] : ROI별 MeanDice 평균"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d3c2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if local_rank == 0:\n",
    "    print(evaluator.state.metrics)\n",
    "    results = evaluator.state.metric_details[\"val_mean_dice\"]\n",
    "    if num_classes > 2:\n",
    "        for i in range(num_classes - 1):\n",
    "            print(\n",
    "                \"mean dice for label {} is {}\".format(i + 1, results[:, i].mean())\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d491afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64228afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13caf267",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(evaluator.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370bea0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(evaluator.state.metric_details), evaluator.state.metric_details.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c84fdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.state.metric_details['val_mean_dice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4288d21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.state.metric_details['val_mean_dice'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336493ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1c774d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdcc675",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4157bd4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
