{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a105b425",
   "metadata": {},
   "source": [
    "## Evaluation 4D dynunet pipeline with NeuroI ROI dataset\n",
    "* `train.py`with mode to `val` 을 기반으로함\n",
    "* `commands/val.sh` 을 참고\n",
    "* Brain image + mask image -> 4D modalities\n",
    "* in_channels: 4, out_channels:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cf9fd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "from monai.utils import first\n",
    "from monai.handlers import (\n",
    "    MeanDice,\n",
    "    from_engine,\n",
    ")\n",
    "from monai.inferers import SimpleInferer, SlidingWindowInferer\n",
    "# from monai.utils import set_determinism\n",
    "\n",
    "from create_network import get_network_ke\n",
    "# from evaluator import (\n",
    "#     DynUNetEvaluator,\n",
    "#     DynUNetEvaluator_gpu,\n",
    "#     DynUNetEvaluator_SaveResult,\n",
    "#     DynUNetEvaluator_GPU_SaveResult_PostMapping\n",
    "# )\n",
    "from config import get_config\n",
    "from dataset_roi_4d import get_val_loader   # 4D modality test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1d1ce8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = \"/data/kehyeong/project/MONAI_examples/dynunet_pipeline/config_roi_earlystop_toy_220209.yaml\"\n",
    "checkpoint = \"/data/train/running/l/model_roi_try1_220217/models/net_key_metric=0.3331.pt\"\n",
    "val_dataset = \"/work/NeuroI-models/ke-monai/data/roi/dataset_test_roi_toy2.csv\"\n",
    "# val_output_dir = \"./runs_eval2\"\n",
    "val_output_dir = \"/home/kehyeong/tmp_result\"\n",
    "\n",
    "# tta_val = True    # ?!?!?!  whether to use test time augmentation.\n",
    "tta_val = False\n",
    "\n",
    "multi_gpu_flag = False\n",
    "spacing = [1.0, 1.0, 1.0]\n",
    "deep_supr_num = 3\n",
    "window_mode = \"gaussian\"     # the mode parameter for SlidingWindowInferer.\n",
    "eval_overlap = 0.5\n",
    "amp = False\n",
    "local_rank = 0\n",
    "\n",
    "config = get_config(config)\n",
    "data_dir = config[\"data_dir\"]\n",
    "image_file_path = config[\"image_file_path\"]\n",
    "label_file_path = config[\"label_file_path\"]\n",
    "mask_file_path = config[\"mask_file_path\"]\n",
    "val_batch_size = config[\"val\"][\"batch_size\"] \n",
    "val_num_workers = config[\"val\"][\"num_workers\"]\n",
    "num_classes = config[\"num_classes\"]\n",
    "patch_size = config[\"patch_size\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5ac381",
   "metadata": {},
   "source": [
    "TTA는 말 그대로 Inference(Test) 과정에서 Augmentation 을 적용한 뒤 예측의 확률을 평균(또는 다른 방법)을 통해 도출하는 기법입니다. 모델 학습간에 다양한 Augmentation 을 적용하여 학습하였을시, Inference 과정에서도 유사한 Augmentation 을 적용하여 정확도를 높일 수 있습니다. 또는 이미지에 객체가 너무 작게 위치한 경우 원본 이미지, Crop 이미지를 넣는 등 다양하게 활용이 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18bd7968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{dir}/s_{id}_{aid}_b.nii.gz /data/train/running/l/input_augmented SU0303 00_0\n",
      "{dir}/s_{id}_{aid}_b.nii.gz /data/train/running/l/input_augmented SU0197 00_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|███████████████████████████████████████████████████████████████████████████| 2/2 [00:08<00:00,  4.01s/it]\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(val_output_dir):\n",
    "    os.makedirs(val_output_dir)\n",
    "    \n",
    "if multi_gpu_flag:\n",
    "    dist.init_process_group(backend=\"nccl\", init_method=\"env://\")\n",
    "    device = torch.device(f\"cuda:{local_rank}\")\n",
    "    torch.cuda.set_device(device)\n",
    "else:\n",
    "    device = torch.device(\"cuda:6\")\n",
    "#     device = torch.device(\"cpu\")\n",
    "\n",
    "val_loader = get_val_loader(\n",
    "    data_dir=data_dir,\n",
    "    id_file=val_dataset,\n",
    "    image_file_pattern=image_file_path,\n",
    "    label_file_pattern=label_file_path,\n",
    "    mask_file_pattern=mask_file_path,\n",
    "    batch_size=val_batch_size,\n",
    "    num_workers=val_num_workers,\n",
    "    multi_gpu_flag=multi_gpu_flag\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d8d65d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test_data = first(val_loader)\n",
    "# print(test_data.keys())\n",
    "# print(\"image shape:\", test_data['image'].shape)\n",
    "# print(\"image dtype:\", test_data['image'].dtype)\n",
    "# print(\"label shape:\", test_data['label'].shape)\n",
    "# print(\"label dtype:\", test_data['label'].dtype)\n",
    "# print(\"1번 배치의 유니크한 라벨 리스트:\", np.unique(test_data['label']))\n",
    "# total_labels = np.unique(test_data['label'])\n",
    "# print(f'1번 배치의 유니크한 라벨 class 수: {len(total_labels)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0f817a",
   "metadata": {},
   "source": [
    "------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25814ac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 109)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "properties = {\n",
    "    'modality': [0,1],\n",
    "    'labels': np.arange(num_classes)\n",
    "}\n",
    "n_class = len(properties[\"labels\"])\n",
    "in_channels = len(properties[\"modality\"])\n",
    "in_channels, n_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58a2caf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretrained checkpoint: /data/train/running/l/model_roi_try1_220217/models/net_key_metric=0.3331.pt loaded\n",
      "Loading nnUNET Done!!!\n",
      "Loading nnUNET to GPU devices Done!!!\n",
      "DynUNet(\n",
      "  (input_block): UnetBasicBlock(\n",
      "    (conv1): Convolution(\n",
      "      (conv): Conv3d(2, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "    )\n",
      "    (conv2): Convolution(\n",
      "      (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "    )\n",
      "    (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (norm1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (norm2): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "  )\n",
      "  (downsamples): ModuleList(\n",
      "    (0): UnetBasicBlock(\n",
      "      (conv1): Convolution(\n",
      "        (conv): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "      )\n",
      "      (conv2): Convolution(\n",
      "        (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      )\n",
      "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "      (norm1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      (norm2): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    )\n",
      "    (1): UnetBasicBlock(\n",
      "      (conv1): Convolution(\n",
      "        (conv): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "      )\n",
      "      (conv2): Convolution(\n",
      "        (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      )\n",
      "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "      (norm1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      (norm2): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    )\n",
      "    (2): UnetBasicBlock(\n",
      "      (conv1): Convolution(\n",
      "        (conv): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "      )\n",
      "      (conv2): Convolution(\n",
      "        (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      )\n",
      "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "      (norm1): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      (norm2): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    )\n",
      "  )\n",
      "  (bottleneck): UnetBasicBlock(\n",
      "    (conv1): Convolution(\n",
      "      (conv): Conv3d(256, 320, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "    )\n",
      "    (conv2): Convolution(\n",
      "      (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "    )\n",
      "    (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (norm1): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (norm2): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "  )\n",
      "  (upsamples): ModuleList(\n",
      "    (0): UnetUpBlock(\n",
      "      (transp_conv): Convolution(\n",
      "        (conv): ConvTranspose3d(320, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
      "      )\n",
      "      (conv_block): UnetBasicBlock(\n",
      "        (conv1): Convolution(\n",
      "          (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        )\n",
      "        (conv2): Convolution(\n",
      "          (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        )\n",
      "        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "        (norm1): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (norm2): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (1): UnetUpBlock(\n",
      "      (transp_conv): Convolution(\n",
      "        (conv): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
      "      )\n",
      "      (conv_block): UnetBasicBlock(\n",
      "        (conv1): Convolution(\n",
      "          (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        )\n",
      "        (conv2): Convolution(\n",
      "          (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        )\n",
      "        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "        (norm1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (norm2): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (2): UnetUpBlock(\n",
      "      (transp_conv): Convolution(\n",
      "        (conv): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
      "      )\n",
      "      (conv_block): UnetBasicBlock(\n",
      "        (conv1): Convolution(\n",
      "          (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        )\n",
      "        (conv2): Convolution(\n",
      "          (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        )\n",
      "        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "        (norm1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (norm2): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (3): UnetUpBlock(\n",
      "      (transp_conv): Convolution(\n",
      "        (conv): ConvTranspose3d(64, 32, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
      "      )\n",
      "      (conv_block): UnetBasicBlock(\n",
      "        (conv1): Convolution(\n",
      "          (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        )\n",
      "        (conv2): Convolution(\n",
      "          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        )\n",
      "        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "        (norm1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (norm2): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (output_block): UnetOutBlock(\n",
      "    (conv): Convolution(\n",
      "      (conv): Conv3d(32, 109, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "    )\n",
      "  )\n",
      "  (deep_supervision_heads): ModuleList(\n",
      "    (0): UnetOutBlock(\n",
      "      (conv): Convolution(\n",
      "        (conv): Conv3d(64, 109, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "      )\n",
      "    )\n",
      "    (1): UnetOutBlock(\n",
      "      (conv): Convolution(\n",
      "        (conv): Conv3d(128, 109, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "      )\n",
      "    )\n",
      "    (2): UnetOutBlock(\n",
      "      (conv): Convolution(\n",
      "        (conv): Conv3d(256, 109, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (skip_layers): DynUNetSkipLayer(\n",
      "    (downsample): UnetBasicBlock(\n",
      "      (conv1): Convolution(\n",
      "        (conv): Conv3d(2, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      )\n",
      "      (conv2): Convolution(\n",
      "        (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      )\n",
      "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "      (norm1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      (norm2): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    )\n",
      "    (next_layer): DynUNetSkipLayer(\n",
      "      (downsample): UnetBasicBlock(\n",
      "        (conv1): Convolution(\n",
      "          (conv): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "        )\n",
      "        (conv2): Convolution(\n",
      "          (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        )\n",
      "        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "        (norm1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (norm2): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "      (next_layer): DynUNetSkipLayer(\n",
      "        (downsample): UnetBasicBlock(\n",
      "          (conv1): Convolution(\n",
      "            (conv): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "          )\n",
      "          (conv2): Convolution(\n",
      "            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "          )\n",
      "          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          (norm1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "          (norm2): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        )\n",
      "        (next_layer): DynUNetSkipLayer(\n",
      "          (downsample): UnetBasicBlock(\n",
      "            (conv1): Convolution(\n",
      "              (conv): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "            )\n",
      "            (conv2): Convolution(\n",
      "              (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "            )\n",
      "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "            (norm1): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "            (norm2): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "          )\n",
      "          (next_layer): UnetBasicBlock(\n",
      "            (conv1): Convolution(\n",
      "              (conv): Conv3d(256, 320, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "            )\n",
      "            (conv2): Convolution(\n",
      "              (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "            )\n",
      "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "            (norm1): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "            (norm2): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "          )\n",
      "          (upsample): UnetUpBlock(\n",
      "            (transp_conv): Convolution(\n",
      "              (conv): ConvTranspose3d(320, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
      "            )\n",
      "            (conv_block): UnetBasicBlock(\n",
      "              (conv1): Convolution(\n",
      "                (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "              )\n",
      "              (conv2): Convolution(\n",
      "                (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "              )\n",
      "              (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "              (norm1): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "              (norm2): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "            )\n",
      "          )\n",
      "          (super_head): UnetOutBlock(\n",
      "            (conv): Convolution(\n",
      "              (conv): Conv3d(256, 109, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (upsample): UnetUpBlock(\n",
      "          (transp_conv): Convolution(\n",
      "            (conv): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
      "          )\n",
      "          (conv_block): UnetBasicBlock(\n",
      "            (conv1): Convolution(\n",
      "              (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "            )\n",
      "            (conv2): Convolution(\n",
      "              (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "            )\n",
      "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "            (norm1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "            (norm2): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "          )\n",
      "        )\n",
      "        (super_head): UnetOutBlock(\n",
      "          (conv): Convolution(\n",
      "            (conv): Conv3d(128, 109, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (upsample): UnetUpBlock(\n",
      "        (transp_conv): Convolution(\n",
      "          (conv): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
      "        )\n",
      "        (conv_block): UnetBasicBlock(\n",
      "          (conv1): Convolution(\n",
      "            (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "          )\n",
      "          (conv2): Convolution(\n",
      "            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "          )\n",
      "          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          (norm1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "          (norm2): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        )\n",
      "      )\n",
      "      (super_head): UnetOutBlock(\n",
      "        (conv): Convolution(\n",
      "          (conv): Conv3d(64, 109, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (upsample): UnetUpBlock(\n",
      "      (transp_conv): Convolution(\n",
      "        (conv): ConvTranspose3d(64, 32, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
      "      )\n",
      "      (conv_block): UnetBasicBlock(\n",
      "        (conv1): Convolution(\n",
      "          (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        )\n",
      "        (conv2): Convolution(\n",
      "          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        )\n",
      "        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "        (norm1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (norm2): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# produce the network\n",
    "net = get_network_ke(properties, patch_size, spacing, deep_supr_num, \n",
    "                     val_output_dir, checkpoint)    # val_output_dir은 삭제 필요. 안씀\n",
    "print('Loading nnUNET Done!!!')\n",
    "net = net.to(device)\n",
    "print('Loading nnUNET to GPU devices Done!!!')\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f256c681",
   "metadata": {},
   "outputs": [],
   "source": [
    "if multi_gpu_flag:\n",
    "    net = DistributedDataParallel(module=net, device_ids=[device])\n",
    "\n",
    "num_classes = len(properties[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "352432b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=6)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4524af9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 1. 기본형\n",
    "# net.eval()\n",
    "# evaluator = DynUNetEvaluator(\n",
    "#     device=device,\n",
    "#     val_data_loader=val_loader,\n",
    "#     network=net,\n",
    "#     num_classes=num_classes,\n",
    "#     inferer=SlidingWindowInferer(\n",
    "#         roi_size=patch_size,\n",
    "#         sw_batch_size=val_batch_size,\n",
    "#         overlap=eval_overlap,\n",
    "#         mode=window_mode,\n",
    "#     ),\n",
    "#     postprocessing=None,      # 이걸 바꿔줘야할듯...\n",
    "#     key_val_metric={\n",
    "#         \"val_mean_dice\": MeanDice(\n",
    "#             include_background=False,\n",
    "#             output_transform=from_engine([\"pred\", \"label\"]),\n",
    "#         )\n",
    "#     },\n",
    "#     additional_metrics=None,\n",
    "#     amp=amp,\n",
    "#     tta_val=tta_val,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2e6648c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 2. GPU inference 형\n",
    "# net.eval()\n",
    "# evaluator = DynUNetEvaluator_gpu(\n",
    "#     device=device,\n",
    "#     val_data_loader=val_loader,\n",
    "#     network=net,\n",
    "#     num_classes=num_classes,\n",
    "#     inferer=SlidingWindowInferer(\n",
    "#         roi_size=patch_size,\n",
    "#         sw_batch_size=val_batch_size,\n",
    "#         overlap=eval_overlap,\n",
    "#         mode=window_mode,\n",
    "#     ),\n",
    "#     postprocessing=None,      # 이걸 바꿔줘야할듯...\n",
    "#     key_val_metric={\n",
    "#         \"val_mean_dice\": MeanDice(\n",
    "#             include_background=False,\n",
    "#             output_transform=from_engine([\"pred\", \"label\"]),\n",
    "#         )\n",
    "#     },\n",
    "#     additional_metrics=None,\n",
    "#     amp=amp,\n",
    "#     tta_val=tta_val,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "027b8862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 3. GPU ver. + inference image 저장 + tta False\n",
    "\n",
    "# net.eval()\n",
    "# evaluator = DynUNetEvaluator_SaveResult(\n",
    "#     device=device,\n",
    "#     val_data_loader=val_loader,\n",
    "#     network=net,\n",
    "#     output_dir=val_output_dir,\n",
    "#     num_classes=num_classes,\n",
    "#     inferer=SlidingWindowInferer(\n",
    "#         roi_size=patch_size,\n",
    "#         sw_batch_size=val_batch_size,\n",
    "#         overlap=eval_overlap,\n",
    "#         mode=window_mode,\n",
    "#     ),\n",
    "#     postprocessing=None,      # 이걸 바꿔줘야할듯...\n",
    "#     key_val_metric={\n",
    "#         \"val_mean_dice\": MeanDice(\n",
    "#             include_background=False,\n",
    "#             output_transform=from_engine([\"pred\", \"label\"]),\n",
    "#         )\n",
    "#     },\n",
    "#     additional_metrics=None,\n",
    "#     amp=amp,\n",
    "#     tta_val=tta_val,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "010943c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 4. GPU ver. + inference image 저장형 + tta False + posttransform (mapping index)\n",
    "# from monai.transforms import (\n",
    "#     LoadImaged,\n",
    "#     AddChanneld,\n",
    "#     MapLabelValued,\n",
    "#     Compose\n",
    "# )\n",
    "\n",
    "# orig_label_classes, target_label_classes = (\n",
    "#     np.array([   0,    2,    3,    4,    5,    7,    8,   10,   11,   12,   13,\n",
    "#          14,   15,   16,   17,   18,   24,   26,   28,   30,   31,   41,\n",
    "#          42,   43,   44,   46,   47,   49,   50,   51,   52,   53,   54,\n",
    "#          58,   60,   62,   63,   77,   80,   85,  251,  252,  253,  254,\n",
    "#         255, 1000, 1002, 1003, 1005, 1006, 1007, 1008, 1009, 1010, 1011,\n",
    "#        1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022,\n",
    "#        1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1034, 1035,\n",
    "#        2000, 2002, 2003, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012,\n",
    "#        2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023,\n",
    "#        2024, 2025, 2026, 2027, 2028, 2029, 2030, 2031, 2034, 2035]),\n",
    "#     np.array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
    "#         13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
    "#         26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
    "#         39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
    "#         52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
    "#         65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
    "#         78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
    "#         91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
    "#        104, 105, 106, 107, 108])\n",
    "# )\n",
    "\n",
    "# post_trans = MapLabelValued(\n",
    "#     keys=[\"pred\"], \n",
    "#     orig_labels=target_label_classes, \n",
    "#     target_labels=orig_label_classes\n",
    "# )\n",
    "\n",
    "\n",
    "# net.eval()\n",
    "# evaluator = DynUNetEvaluator_SaveResult(\n",
    "#     device=device,\n",
    "#     val_data_loader=val_loader,\n",
    "#     network=net,\n",
    "#     output_dir=val_output_dir,\n",
    "#     num_classes=num_classes,\n",
    "#     inferer=SlidingWindowInferer(\n",
    "#         roi_size=patch_size,\n",
    "#         sw_batch_size=val_batch_size,\n",
    "#         overlap=eval_overlap,\n",
    "#         mode=window_mode,\n",
    "#     ),\n",
    "#     postprocessing=post_trans,      # 이걸 바꿔줘야할듯...\n",
    "#     key_val_metric={\n",
    "#         \"val_mean_dice\": MeanDice(\n",
    "#             include_background=False,\n",
    "#             output_transform=from_engine([\"pred\", \"label\"]),\n",
    "#         )\n",
    "#     },\n",
    "#     additional_metrics=None,\n",
    "#     amp=amp,\n",
    "#     tta_val=tta_val,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f823b6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from typing import Any, Callable, Dict, Optional, Sequence, Tuple, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from ignite.engine import Engine\n",
    "from ignite.metrics import Metric\n",
    "from monai.data import decollate_batch\n",
    "from monai.data.nifti_writer import write_nifti\n",
    "from monai.engines import SupervisedEvaluator\n",
    "from monai.engines.utils import CommonKeys as Keys\n",
    "from monai.engines.utils import IterationEvents, default_prepare_batch\n",
    "from monai.inferers import Inferer\n",
    "from monai.networks.utils import eval_mode\n",
    "from monai.transforms import AsDiscrete, Transform\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transforms import recovery_prediction\n",
    "\n",
    "from monai.transforms import Compose, AddChannel, MapLabelValue, CastToType\n",
    "\n",
    "class DynUNetEvaluator_GPU_SaveResult_PostMapping(SupervisedEvaluator):\n",
    "    \"\"\"\n",
    "    넣어준 모든 이미지들의 DiceMean 계산 뿐 아니라 inference 결과 이미지도 저장하도록 변경\n",
    "\n",
    "    Args:\n",
    "        device: an object representing the device on which to run.\n",
    "        val_data_loader: Ignite engine use data_loader to run, must be\n",
    "            torch.DataLoader.\n",
    "        network: use the network to run model forward.\n",
    "        num_classes: the number of classes (output channels) for the task.\n",
    "        epoch_length: number of iterations for one epoch, default to\n",
    "            `len(val_data_loader)`.\n",
    "        non_blocking: if True and this copy is between CPU and GPU, the copy may occur asynchronously\n",
    "            with respect to the host. For other cases, this argument has no effect.\n",
    "        prepare_batch: function to parse image and label for current iteration.\n",
    "        iteration_update: the callable function for every iteration, expect to accept `engine`\n",
    "            and `batchdata` as input parameters. if not provided, use `self._iteration()` instead.\n",
    "        inferer: inference method that execute model forward on input data, like: SlidingWindow, etc.\n",
    "        postprocessing: execute additional transformation for the model output data.\n",
    "            Typically, several Tensor based transforms composed by `Compose`.\n",
    "        key_val_metric: compute metric when every iteration completed, and save average value to\n",
    "            engine.state.metrics when epoch completed. key_val_metric is the main metric to compare and save the\n",
    "            checkpoint into files.\n",
    "        additional_metrics: more Ignite metrics that also attach to Ignite Engine.\n",
    "        val_handlers: every handler is a set of Ignite Event-Handlers, must have `attach` function, like:\n",
    "            CheckpointHandler, StatsHandler, SegmentationSaver, etc.\n",
    "        amp: whether to enable auto-mixed-precision evaluation, default is False.\n",
    "        tta_val: whether to do the 8 flips (8 = 2 ** 3, where 3 represents the three dimensions)\n",
    "            test time augmentation, default is False.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        device: torch.device,\n",
    "        val_data_loader: DataLoader,\n",
    "        network: torch.nn.Module,\n",
    "        output_dir: str,                                      # infer specific\n",
    "        num_classes: Union[str, int],\n",
    "        epoch_length: Optional[int] = None,\n",
    "        non_blocking: bool = False,\n",
    "        prepare_batch: Callable = default_prepare_batch,\n",
    "        iteration_update: Optional[Callable] = None,\n",
    "        inferer: Optional[Inferer] = None,\n",
    "        postprocessing: Optional[Transform] = None,\n",
    "        key_val_metric: Optional[Dict[str, Metric]] = None,\n",
    "        additional_metrics: Optional[Dict[str, Metric]] = None,\n",
    "        val_handlers: Optional[Sequence] = None,\n",
    "        amp: bool = False,\n",
    "        tta_val: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__(\n",
    "            device=device,\n",
    "            val_data_loader=val_data_loader,\n",
    "            network=network,\n",
    "            epoch_length=epoch_length,\n",
    "            non_blocking=non_blocking,\n",
    "            prepare_batch=prepare_batch,\n",
    "            iteration_update=iteration_update,\n",
    "            inferer=inferer,\n",
    "            postprocessing=postprocessing,\n",
    "            key_val_metric=key_val_metric,\n",
    "            additional_metrics=additional_metrics,\n",
    "            val_handlers=val_handlers,\n",
    "            amp=amp,\n",
    "        )\n",
    "\n",
    "        if not isinstance(num_classes, int):\n",
    "            num_classes = int(num_classes)\n",
    "        self.output_dir = output_dir               # infer specific\n",
    "        self.num_classes = num_classes\n",
    "        self.post_pred = AsDiscrete(argmax=True, to_onehot=num_classes)\n",
    "        self.post_label = AsDiscrete(to_onehot=num_classes)              # eval specific\n",
    "        self.tta_val = tta_val\n",
    "        \n",
    "        # orig_label_classes, target_label_classes = (\n",
    "        #     np.array([   0,    2,    3,    4,    5,    7,    8,   10,   11,   12,   13,\n",
    "        #         14,   15,   16,   17,   18,   24,   26,   28,   30,   31,   41,\n",
    "        #         42,   43,   44,   46,   47,   49,   50,   51,   52,   53,   54,\n",
    "        #         58,   60,   62,   63,   77,   80,   85,  251,  252,  253,  254,\n",
    "        #         255, 1000, 1002, 1003, 1005, 1006, 1007, 1008, 1009, 1010, 1011,\n",
    "        #     1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022,\n",
    "        #     1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1034, 1035,\n",
    "        #     2000, 2002, 2003, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012,\n",
    "        #     2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023,\n",
    "        #     2024, 2025, 2026, 2027, 2028, 2029, 2030, 2031, 2034, 2035], dtype=np.float64),\n",
    "        #     np.array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
    "        #         13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
    "        #         26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
    "        #         39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
    "        #         52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
    "        #         65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
    "        #         78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
    "        #         91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
    "        #     104, 105, 106, 107, 108], dtype=np.float64)\n",
    "        # )\n",
    "        # self.orig_label_classes = orig_label_classes\n",
    "        # self.target_label_classes = target_label_classes\n",
    "        # self.post_trans = Compose([\n",
    "        #     AddChannel(),\n",
    "        #     MapLabelValue(    # 0~108로 잡혀있는 label을 original index로 변경\n",
    "        #                   orig_labels=self.target_label_classes, \n",
    "        #                   target_labels=self.orig_label_classes,\n",
    "        #                   dtype=np.uint8\n",
    "        #                   )\n",
    "        #     ])\n",
    "        # self.post_trans = MapLabelValue(    # 0~108로 잡혀있는 label을 original index로 변경\n",
    "        #                                 orig_labels=self.target_label_classes, \n",
    "        #                                 target_labels=self.orig_label_classes\n",
    "        #                   )\n",
    "\n",
    "    def _iteration(\n",
    "        self, engine: Engine, batchdata: Dict[str, Any]\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        callback function for the Supervised Evaluation processing logic of 1 iteration in Ignite Engine.\n",
    "        Return below items in a dictionary:\n",
    "            - IMAGE: image Tensor data for model input, already moved to device.\n",
    "            - LABEL: label Tensor data corresponding to the image, already moved to device.\n",
    "            - PRED: prediction result of model.\n",
    "\n",
    "        Args:\n",
    "            engine: Ignite Engine, it can be a trainer, validator or evaluator.\n",
    "            batchdata: input data for this iteration, usually can be dictionary or tuple of Tensor data.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: When ``batchdata`` is None.\n",
    "\n",
    "        \"\"\"\n",
    "        if batchdata is None:\n",
    "            raise ValueError(\"Must provide batch data for current iteration.\")\n",
    "        batch = self.prepare_batch(batchdata, engine.state.device, engine.non_blocking)\n",
    "        if len(batch) == 2:\n",
    "            inputs, targets = batch\n",
    "            args: Tuple = ()\n",
    "            kwargs: Dict = {}\n",
    "        else:\n",
    "            inputs, targets, args, kwargs = batch\n",
    "\n",
    "        targets = targets.cpu()      # device로 바꿔보자\n",
    "        # print('CPU 비활성화!!!!!!!!!! ')\n",
    "\n",
    "        def _compute_pred():\n",
    "            ct = 1.0\n",
    "            # pred = self.inferer(inputs, self.network, *args, **kwargs).cpu()    # device로 바꿔보자 (지우면댐)\n",
    "            pred = self.inferer(inputs, self.network, *args, **kwargs)    # device로 바꿔보자 (지우면댐)\n",
    "            pred = nn.functional.softmax(pred, dim=1)\n",
    "            if not self.tta_val:\n",
    "                return pred\n",
    "            else:\n",
    "                for dims in [[2], [3], [4], (2, 3), (2, 4), (3, 4), (2, 3, 4)]:\n",
    "                    flip_inputs = torch.flip(inputs, dims=dims)\n",
    "                    flip_pred = torch.flip(\n",
    "                        # self.inferer(flip_inputs, self.network).cpu(), dims=dims    # device로 바꿔보자\n",
    "                        self.inferer(flip_inputs, self.network), dims=dims    # device로 바꿔보자\n",
    "                    )\n",
    "                    flip_pred = nn.functional.softmax(flip_pred, dim=1)\n",
    "                    del flip_inputs\n",
    "                    pred += flip_pred\n",
    "                    del flip_pred\n",
    "                    ct += 1\n",
    "                return pred / ct\n",
    "\n",
    "        # execute forward computation\n",
    "        with eval_mode(self.network):\n",
    "            if self.amp:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    predictions = _compute_pred()\n",
    "            else:\n",
    "                predictions = _compute_pred()\n",
    "\n",
    "        inputs = inputs.cpu()    #  # device로 바꿔보자\n",
    "\n",
    "        predictions = self.post_pred(decollate_batch(predictions)[0])\n",
    "        targets = self.post_label(decollate_batch(targets)[0])                # eval specific\n",
    "\n",
    "        affine = batchdata[\"image_meta_dict\"][\"affine\"].numpy()[0]            # infer specific\n",
    "        resample_flag = batchdata[\"resample_flag\"]\n",
    "        anisotrophy_flag = batchdata[\"anisotrophy_flag\"]\n",
    "        crop_shape = batchdata[\"crop_shape\"][0].tolist()\n",
    "        original_shape = batchdata[\"original_shape\"][0].tolist()\n",
    "        if resample_flag:\n",
    "            # convert the prediction back to the original (after cropped) shape\n",
    "            predictions = recovery_prediction(\n",
    "                predictions.numpy(), [self.num_classes, *crop_shape], anisotrophy_flag\n",
    "            )\n",
    "            predictions = torch.tensor(predictions)\n",
    "\n",
    "        ## 이미지 저장\n",
    "        predictions_wirte = predictions.cpu()\n",
    "        print(type(predictions_wirte))\n",
    "        print(predictions_wirte.shape)\n",
    "        predictions_wirte = np.argmax(predictions_wirte, axis=0)\n",
    "        print(predictions_wirte.shape)\n",
    "        predictions_wirte_org = np.zeros([*original_shape])\n",
    "        \n",
    "        # put iteration outputs into engine.state\n",
    "        engine.state.output = {Keys.IMAGE: inputs, Keys.LABEL: targets.unsqueeze(0)}\n",
    "        engine.state.output[Keys.PRED] = torch.zeros([1, self.num_classes, *original_shape])\n",
    "        # pad the prediction back to the original shape\n",
    "        box_start, box_end = batchdata[\"bbox\"][0]\n",
    "        h_start, w_start, d_start = box_start\n",
    "        h_end, w_end, d_end = box_end\n",
    "\n",
    "        engine.state.output[Keys.PRED][\n",
    "            0, :, h_start:h_end, w_start:w_end, d_start:d_end\n",
    "        ] = predictions\n",
    "        del predictions\n",
    "\n",
    "        \n",
    "        ## 이미지 저장\n",
    "        # predictions_wirte = self.post_trans(predictions_wirte)\n",
    "        orig_label_classes, target_label_classes = (\n",
    "            np.array([   0,    2,    3,    4,    5,    7,    8,   10,   11,   12,   13,\n",
    "                14,   15,   16,   17,   18,   24,   26,   28,   30,   31,   41,\n",
    "                42,   43,   44,   46,   47,   49,   50,   51,   52,   53,   54,\n",
    "                58,   60,   62,   63,   77,   80,   85,  251,  252,  253,  254,\n",
    "                255, 1000, 1002, 1003, 1005, 1006, 1007, 1008, 1009, 1010, 1011,\n",
    "            1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022,\n",
    "            1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1034, 1035,\n",
    "            2000, 2002, 2003, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012,\n",
    "            2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023,\n",
    "            2024, 2025, 2026, 2027, 2028, 2029, 2030, 2031, 2034, 2035]),\n",
    "            np.array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
    "                13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
    "                26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
    "                39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
    "                52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
    "                65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
    "                78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
    "                91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
    "            104, 105, 106, 107, 108])\n",
    "        )\n",
    "        post_trans = MapLabelValue(    # 0~108로 잡혀있는 label을 original index로 변경\n",
    "            orig_labels=target_label_classes, \n",
    "            target_labels=orig_label_classes,\n",
    "#             dtype=np.uint8\n",
    "        )\n",
    "#         post_trans = Compose([\n",
    "#             MapLabelValue(    # 0~108로 잡혀있는 label을 original index로 변경\n",
    "#             orig_labels=target_label_classes, \n",
    "#             target_labels=orig_label_classes,\n",
    "# #             dtype=np.uint8\n",
    "#             ),\n",
    "#             CastToType(dtype=np.int64)\n",
    "            \n",
    "#         ])\n",
    "               \n",
    "        \n",
    "        predictions_wirte_org[h_start:h_end, w_start:w_end, d_start:d_end] = predictions_wirte\n",
    "        del predictions_wirte\n",
    "        print('변형전 dtype', predictions_wirte_org.dtype)\n",
    "        print(np.unique(predictions_wirte_org))\n",
    "        # predictions_wirte_org = self.post_trans(predictions_wirte_org)   # 원래대로 pred index 복구\n",
    "        # predictions_wirte_org = predictions_wirte_org.squeeze()\n",
    "        predictions_wirte_org = post_trans(predictions_wirte_org)\n",
    "        print('변형후 dtype', predictions_wirte_org.dtype)\n",
    "        print(np.unique(predictions_wirte_org))\n",
    "        \n",
    "        filename = batchdata[\"image_meta_dict\"][\"filename_or_obj\"][0].split(\"/\")[-1]\n",
    "        print(\n",
    "            \"save {} with shape: {}\".format(\n",
    "                filename, predictions_wirte_org.shape\n",
    "            )\n",
    "        )\n",
    "        write_nifti(\n",
    "            data=predictions_wirte_org,\n",
    "            file_name=os.path.join(self.output_dir, filename),\n",
    "            affine=affine,\n",
    "            resample=False,\n",
    "            output_dtype=np.uint32,\n",
    "        )\n",
    "        print(f'MRI img:{ filename } eval done .................')\n",
    "        engine.fire_event(IterationEvents.FORWARD_COMPLETED)\n",
    "        engine.fire_event(IterationEvents.MODEL_COMPLETED)\n",
    "\n",
    "        return engine.state.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0da258",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from typing import Any, Callable, Dict, Optional, Sequence, Tuple, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from ignite.engine import Engine\n",
    "from ignite.metrics import Metric\n",
    "from monai.data import decollate_batch\n",
    "from monai.data.nifti_writer import write_nifti\n",
    "from monai.engines import SupervisedEvaluator\n",
    "from monai.engines.utils import CommonKeys as Keys\n",
    "from monai.engines.utils import IterationEvents, default_prepare_batch\n",
    "from monai.inferers import Inferer\n",
    "from monai.networks.utils import eval_mode\n",
    "from monai.transforms import AsDiscrete, Transform\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transforms import recovery_prediction\n",
    "\n",
    "from monai.transforms import Compose, AddChannel, MapLabelValue, CastToType\n",
    "\n",
    "class DynUNetEvaluator_GPU_SaveResult_PostMapping2(SupervisedEvaluator):\n",
    "    \"\"\"\n",
    "    넣어준 모든 이미지들의 DiceMean 계산 뿐 아니라 inference 결과 이미지도 저장하도록 변경\n",
    "\n",
    "    Args:\n",
    "        device: an object representing the device on which to run.\n",
    "        val_data_loader: Ignite engine use data_loader to run, must be\n",
    "            torch.DataLoader.\n",
    "        network: use the network to run model forward.\n",
    "        num_classes: the number of classes (output channels) for the task.\n",
    "        epoch_length: number of iterations for one epoch, default to\n",
    "            `len(val_data_loader)`.\n",
    "        non_blocking: if True and this copy is between CPU and GPU, the copy may occur asynchronously\n",
    "            with respect to the host. For other cases, this argument has no effect.\n",
    "        prepare_batch: function to parse image and label for current iteration.\n",
    "        iteration_update: the callable function for every iteration, expect to accept `engine`\n",
    "            and `batchdata` as input parameters. if not provided, use `self._iteration()` instead.\n",
    "        inferer: inference method that execute model forward on input data, like: SlidingWindow, etc.\n",
    "        postprocessing: execute additional transformation for the model output data.\n",
    "            Typically, several Tensor based transforms composed by `Compose`.\n",
    "        key_val_metric: compute metric when every iteration completed, and save average value to\n",
    "            engine.state.metrics when epoch completed. key_val_metric is the main metric to compare and save the\n",
    "            checkpoint into files.\n",
    "        additional_metrics: more Ignite metrics that also attach to Ignite Engine.\n",
    "        val_handlers: every handler is a set of Ignite Event-Handlers, must have `attach` function, like:\n",
    "            CheckpointHandler, StatsHandler, SegmentationSaver, etc.\n",
    "        amp: whether to enable auto-mixed-precision evaluation, default is False.\n",
    "        tta_val: whether to do the 8 flips (8 = 2 ** 3, where 3 represents the three dimensions)\n",
    "            test time augmentation, default is False.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        device: torch.device,\n",
    "        val_data_loader: DataLoader,\n",
    "        network: torch.nn.Module,\n",
    "        output_dir: str,                                      # infer specific\n",
    "        num_classes: Union[str, int],\n",
    "        epoch_length: Optional[int] = None,\n",
    "        non_blocking: bool = False,\n",
    "        prepare_batch: Callable = default_prepare_batch,\n",
    "        iteration_update: Optional[Callable] = None,\n",
    "        inferer: Optional[Inferer] = None,\n",
    "        postprocessing: Optional[Transform] = None,\n",
    "        key_val_metric: Optional[Dict[str, Metric]] = None,\n",
    "        additional_metrics: Optional[Dict[str, Metric]] = None,\n",
    "        val_handlers: Optional[Sequence] = None,\n",
    "        amp: bool = False,\n",
    "        tta_val: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__(\n",
    "            device=device,\n",
    "            val_data_loader=val_data_loader,\n",
    "            network=network,\n",
    "            epoch_length=epoch_length,\n",
    "            non_blocking=non_blocking,\n",
    "            prepare_batch=prepare_batch,\n",
    "            iteration_update=iteration_update,\n",
    "            inferer=inferer,\n",
    "            postprocessing=postprocessing,\n",
    "            key_val_metric=key_val_metric,\n",
    "            additional_metrics=additional_metrics,\n",
    "            val_handlers=val_handlers,\n",
    "            amp=amp,\n",
    "        )\n",
    "\n",
    "        if not isinstance(num_classes, int):\n",
    "            num_classes = int(num_classes)\n",
    "        self.output_dir = output_dir               # infer specific\n",
    "        self.num_classes = num_classes\n",
    "        self.post_pred = AsDiscrete(argmax=True, to_onehot=num_classes)\n",
    "        self.post_label = AsDiscrete(to_onehot=num_classes)              # eval specific\n",
    "        self.tta_val = tta_val\n",
    "        \n",
    "        orig_label_classes, target_label_classes = (\n",
    "            np.array([   0,    2,    3,    4,    5,    7,    8,   10,   11,   12,   13,\n",
    "                14,   15,   16,   17,   18,   24,   26,   28,   30,   31,   41,\n",
    "                42,   43,   44,   46,   47,   49,   50,   51,   52,   53,   54,\n",
    "                58,   60,   62,   63,   77,   80,   85,  251,  252,  253,  254,\n",
    "                255, 1000, 1002, 1003, 1005, 1006, 1007, 1008, 1009, 1010, 1011,\n",
    "            1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022,\n",
    "            1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1034, 1035,\n",
    "            2000, 2002, 2003, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012,\n",
    "            2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023,\n",
    "            2024, 2025, 2026, 2027, 2028, 2029, 2030, 2031, 2034, 2035], dtype=np.float64),\n",
    "            np.array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
    "                13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
    "                26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
    "                39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
    "                52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
    "                65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
    "                78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
    "                91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
    "            104, 105, 106, 107, 108], dtype=np.float64)\n",
    "        )\n",
    "        self.orig_label_classes = orig_label_classes\n",
    "        self.target_label_classes = target_label_classes\n",
    "        # self.post_trans = Compose([\n",
    "        #     AddChannel(),\n",
    "        #     MapLabelValue(    # 0~108로 잡혀있는 label을 original index로 변경\n",
    "        #                   orig_labels=self.target_label_classes, \n",
    "        #                   target_labels=self.orig_label_classes,\n",
    "        #                   dtype=np.uint8\n",
    "        #                   )\n",
    "        #     ])\n",
    "        self.post_trans = MapLabelValue(    # 0~108로 잡혀있는 label을 original index로 변경\n",
    "                                        orig_labels=self.target_label_classes, \n",
    "                                        target_labels=self.orig_label_classes\n",
    "                          )\n",
    "\n",
    "    def _iteration(\n",
    "        self, engine: Engine, batchdata: Dict[str, Any]\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        callback function for the Supervised Evaluation processing logic of 1 iteration in Ignite Engine.\n",
    "        Return below items in a dictionary:\n",
    "            - IMAGE: image Tensor data for model input, already moved to device.\n",
    "            - LABEL: label Tensor data corresponding to the image, already moved to device.\n",
    "            - PRED: prediction result of model.\n",
    "\n",
    "        Args:\n",
    "            engine: Ignite Engine, it can be a trainer, validator or evaluator.\n",
    "            batchdata: input data for this iteration, usually can be dictionary or tuple of Tensor data.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: When ``batchdata`` is None.\n",
    "\n",
    "        \"\"\"\n",
    "        if batchdata is None:\n",
    "            raise ValueError(\"Must provide batch data for current iteration.\")\n",
    "        batch = self.prepare_batch(batchdata, engine.state.device, engine.non_blocking)\n",
    "        if len(batch) == 2:\n",
    "            inputs, targets = batch\n",
    "            args: Tuple = ()\n",
    "            kwargs: Dict = {}\n",
    "        else:\n",
    "            inputs, targets, args, kwargs = batch\n",
    "\n",
    "        targets = targets.cpu()      # device로 바꿔보자\n",
    "        # print('CPU 비활성화!!!!!!!!!! ')\n",
    "\n",
    "        def _compute_pred():\n",
    "            ct = 1.0\n",
    "            # pred = self.inferer(inputs, self.network, *args, **kwargs).cpu()    # device로 바꿔보자 (지우면댐)\n",
    "            pred = self.inferer(inputs, self.network, *args, **kwargs)    # device로 바꿔보자 (지우면댐)\n",
    "            pred = nn.functional.softmax(pred, dim=1)\n",
    "            if not self.tta_val:\n",
    "                return pred\n",
    "            else:\n",
    "                for dims in [[2], [3], [4], (2, 3), (2, 4), (3, 4), (2, 3, 4)]:\n",
    "                    flip_inputs = torch.flip(inputs, dims=dims)\n",
    "                    flip_pred = torch.flip(\n",
    "                        # self.inferer(flip_inputs, self.network).cpu(), dims=dims    # device로 바꿔보자\n",
    "                        self.inferer(flip_inputs, self.network), dims=dims    # device로 바꿔보자\n",
    "                    )\n",
    "                    flip_pred = nn.functional.softmax(flip_pred, dim=1)\n",
    "                    del flip_inputs\n",
    "                    pred += flip_pred\n",
    "                    del flip_pred\n",
    "                    ct += 1\n",
    "                return pred / ct\n",
    "\n",
    "        # execute forward computation\n",
    "        with eval_mode(self.network):\n",
    "            if self.amp:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    predictions = _compute_pred()\n",
    "            else:\n",
    "                predictions = _compute_pred()\n",
    "\n",
    "        inputs = inputs.cpu()    #  # device로 바꿔보자\n",
    "\n",
    "        predictions = self.post_pred(decollate_batch(predictions)[0])\n",
    "        targets = self.post_label(decollate_batch(targets)[0])                # eval specific\n",
    "\n",
    "        affine = batchdata[\"image_meta_dict\"][\"affine\"].numpy()[0]            # infer specific\n",
    "        resample_flag = batchdata[\"resample_flag\"]\n",
    "        anisotrophy_flag = batchdata[\"anisotrophy_flag\"]\n",
    "        crop_shape = batchdata[\"crop_shape\"][0].tolist()\n",
    "        original_shape = batchdata[\"original_shape\"][0].tolist()\n",
    "        if resample_flag:\n",
    "            # convert the prediction back to the original (after cropped) shape\n",
    "            predictions = recovery_prediction(\n",
    "                predictions.numpy(), [self.num_classes, *crop_shape], anisotrophy_flag\n",
    "            )\n",
    "            predictions = torch.tensor(predictions)\n",
    "\n",
    "        ## 이미지 저장\n",
    "        predictions_wirte = predictions.cpu()\n",
    "        print(type(predictions_wirte))\n",
    "        print(predictions_wirte.shape)\n",
    "        predictions_wirte = np.argmax(predictions_wirte, axis=0)\n",
    "        print(predictions_wirte.shape)\n",
    "        predictions_wirte_org = np.zeros([*original_shape])\n",
    "        \n",
    "        # put iteration outputs into engine.state\n",
    "        engine.state.output = {Keys.IMAGE: inputs, Keys.LABEL: targets.unsqueeze(0)}\n",
    "        engine.state.output[Keys.PRED] = torch.zeros([1, self.num_classes, *original_shape])\n",
    "        # pad the prediction back to the original shape\n",
    "        box_start, box_end = batchdata[\"bbox\"][0]\n",
    "        h_start, w_start, d_start = box_start\n",
    "        h_end, w_end, d_end = box_end\n",
    "\n",
    "        engine.state.output[Keys.PRED][\n",
    "            0, :, h_start:h_end, w_start:w_end, d_start:d_end\n",
    "        ] = predictions\n",
    "        del predictions\n",
    "\n",
    "        \n",
    "        ## 이미지 저장\n",
    "        # predictions_wirte = self.post_trans(predictions_wirte)\n",
    "#         orig_label_classes, target_label_classes = (\n",
    "#             np.array([   0,    2,    3,    4,    5,    7,    8,   10,   11,   12,   13,\n",
    "#                 14,   15,   16,   17,   18,   24,   26,   28,   30,   31,   41,\n",
    "#                 42,   43,   44,   46,   47,   49,   50,   51,   52,   53,   54,\n",
    "#                 58,   60,   62,   63,   77,   80,   85,  251,  252,  253,  254,\n",
    "#                 255, 1000, 1002, 1003, 1005, 1006, 1007, 1008, 1009, 1010, 1011,\n",
    "#             1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022,\n",
    "#             1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1034, 1035,\n",
    "#             2000, 2002, 2003, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012,\n",
    "#             2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023,\n",
    "#             2024, 2025, 2026, 2027, 2028, 2029, 2030, 2031, 2034, 2035]),\n",
    "#             np.array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
    "#                 13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
    "#                 26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
    "#                 39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
    "#                 52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
    "#                 65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
    "#                 78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
    "#                 91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
    "#             104, 105, 106, 107, 108])\n",
    "#         )\n",
    "#         post_trans = MapLabelValue(    # 0~108로 잡혀있는 label을 original index로 변경\n",
    "#             orig_labels=target_label_classes, \n",
    "#             target_labels=orig_label_classes,\n",
    "#         )\n",
    "#         post_trans = Compose([\n",
    "#             MapLabelValue(    # 0~108로 잡혀있는 label을 original index로 변경\n",
    "#             orig_labels=target_label_classes, \n",
    "#             target_labels=orig_label_classes,\n",
    "# #             dtype=np.uint8\n",
    "#             ),\n",
    "#             CastToType(dtype=np.int64)\n",
    "            \n",
    "#         ])\n",
    "               \n",
    "        \n",
    "        predictions_wirte_org[h_start:h_end, w_start:w_end, d_start:d_end] = predictions_wirte\n",
    "        del predictions_wirte\n",
    "        print('변형전 dtype', predictions_wirte_org.dtype)\n",
    "        print(np.unique(predictions_wirte_org))\n",
    "        predictions_wirte_org = self.post_trans(predictions_wirte_org)   # 원래대로 pred index 복구\n",
    "        # predictions_wirte_org = predictions_wirte_org.squeeze()\n",
    "#         predictions_wirte_org = post_trans(predictions_wirte_org)\n",
    "        print('변형후 dtype', predictions_wirte_org.dtype)\n",
    "        print(np.unique(predictions_wirte_org))\n",
    "        \n",
    "        filename = batchdata[\"image_meta_dict\"][\"filename_or_obj\"][0].split(\"/\")[-1]\n",
    "        print(\n",
    "            \"save {} with shape: {}\".format(\n",
    "                filename, predictions_wirte_org.shape\n",
    "            )\n",
    "        )\n",
    "        write_nifti(\n",
    "            data=predictions_wirte_org,\n",
    "            file_name=os.path.join(self.output_dir, filename),\n",
    "            affine=affine,\n",
    "            resample=False,\n",
    "            output_dtype=np.uint32,\n",
    "        )\n",
    "        print(f'MRI img:{ filename } eval done .................')\n",
    "        engine.fire_event(IterationEvents.FORWARD_COMPLETED)\n",
    "        engine.fire_event(IterationEvents.MODEL_COMPLETED)\n",
    "\n",
    "        return engine.state.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f4e27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5. GPU ver. + inference image 저장형 + tta False + posttransform 2 (mapping index)\n",
    "# evaluator class 내부에서 transform\n",
    "net.eval()\n",
    "evaluator = DynUNetEvaluator_GPU_SaveResult_PostMapping2(\n",
    "    device=device,\n",
    "    val_data_loader=val_loader,\n",
    "    network=net,\n",
    "    output_dir=val_output_dir,\n",
    "    num_classes=num_classes,\n",
    "    inferer=SlidingWindowInferer(\n",
    "        roi_size=patch_size,\n",
    "        sw_batch_size=val_batch_size,\n",
    "        overlap=eval_overlap,\n",
    "        mode=window_mode,\n",
    "    ),\n",
    "    postprocessing=None,\n",
    "    key_val_metric={\n",
    "        \"val_mean_dice\": MeanDice(\n",
    "            include_background=False,\n",
    "            output_transform=from_engine([\"pred\", \"label\"]),\n",
    "        )\n",
    "    },\n",
    "    additional_metrics=None,\n",
    "    amp=amp,\n",
    "    tta_val=tta_val,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a9ceac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "evaluator.run()\n",
    "print(\"time :\", time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "77d4ed43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from typing import Any, Callable, Dict, Optional, Sequence, Tuple, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from ignite.engine import Engine\n",
    "from ignite.metrics import Metric\n",
    "from monai.data import decollate_batch\n",
    "from monai.data.nifti_writer import write_nifti\n",
    "from monai.engines import SupervisedEvaluator\n",
    "from monai.engines.utils import CommonKeys as Keys\n",
    "from monai.engines.utils import IterationEvents, default_prepare_batch\n",
    "from monai.inferers import Inferer\n",
    "from monai.networks.utils import eval_mode\n",
    "from monai.transforms import AsDiscrete, Transform\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transforms import recovery_prediction\n",
    "\n",
    "from monai.transforms import Compose, AddChannel, MapLabelValue, CastToType\n",
    "\n",
    "class DynUNetEvaluator_GPU_test(SupervisedEvaluator):\n",
    "    \"\"\"\n",
    "    넣어준 모든 이미지들의 DiceMean 계산 뿐 아니라 inference 결과 이미지도 저장하도록 변경\n",
    "    GPU inference 테스트\n",
    "\n",
    "    Args:\n",
    "        device: an object representing the device on which to run.\n",
    "        val_data_loader: Ignite engine use data_loader to run, must be\n",
    "            torch.DataLoader.\n",
    "        network: use the network to run model forward.\n",
    "        num_classes: the number of classes (output channels) for the task.\n",
    "        epoch_length: number of iterations for one epoch, default to\n",
    "            `len(val_data_loader)`.\n",
    "        non_blocking: if True and this copy is between CPU and GPU, the copy may occur asynchronously\n",
    "            with respect to the host. For other cases, this argument has no effect.\n",
    "        prepare_batch: function to parse image and label for current iteration.\n",
    "        iteration_update: the callable function for every iteration, expect to accept `engine`\n",
    "            and `batchdata` as input parameters. if not provided, use `self._iteration()` instead.\n",
    "        inferer: inference method that execute model forward on input data, like: SlidingWindow, etc.\n",
    "        postprocessing: execute additional transformation for the model output data.\n",
    "            Typically, several Tensor based transforms composed by `Compose`.\n",
    "        key_val_metric: compute metric when every iteration completed, and save average value to\n",
    "            engine.state.metrics when epoch completed. key_val_metric is the main metric to compare and save the\n",
    "            checkpoint into files.\n",
    "        additional_metrics: more Ignite metrics that also attach to Ignite Engine.\n",
    "        val_handlers: every handler is a set of Ignite Event-Handlers, must have `attach` function, like:\n",
    "            CheckpointHandler, StatsHandler, SegmentationSaver, etc.\n",
    "        amp: whether to enable auto-mixed-precision evaluation, default is False.\n",
    "        tta_val: whether to do the 8 flips (8 = 2 ** 3, where 3 represents the three dimensions)\n",
    "            test time augmentation, default is False.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        device: torch.device,\n",
    "        val_data_loader: DataLoader,\n",
    "        network: torch.nn.Module,\n",
    "        output_dir: str,                                      # infer specific\n",
    "        num_classes: Union[str, int],\n",
    "        epoch_length: Optional[int] = None,\n",
    "        non_blocking: bool = False,\n",
    "        prepare_batch: Callable = default_prepare_batch,\n",
    "        iteration_update: Optional[Callable] = None,\n",
    "        inferer: Optional[Inferer] = None,\n",
    "        postprocessing: Optional[Transform] = None,\n",
    "        key_val_metric: Optional[Dict[str, Metric]] = None,\n",
    "        additional_metrics: Optional[Dict[str, Metric]] = None,\n",
    "        val_handlers: Optional[Sequence] = None,\n",
    "        amp: bool = False,\n",
    "        tta_val: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__(\n",
    "            device=device,\n",
    "            val_data_loader=val_data_loader,\n",
    "            network=network,\n",
    "            epoch_length=epoch_length,\n",
    "            non_blocking=non_blocking,\n",
    "            prepare_batch=prepare_batch,\n",
    "            iteration_update=iteration_update,\n",
    "            inferer=inferer,\n",
    "            postprocessing=postprocessing,\n",
    "            key_val_metric=key_val_metric,\n",
    "            additional_metrics=additional_metrics,\n",
    "            val_handlers=val_handlers,\n",
    "            amp=amp,\n",
    "        )\n",
    "\n",
    "        if not isinstance(num_classes, int):\n",
    "            num_classes = int(num_classes)\n",
    "        self.output_dir = output_dir               # infer specific\n",
    "        self.num_classes = num_classes\n",
    "        self.post_pred = AsDiscrete(argmax=True, to_onehot=num_classes)\n",
    "        self.post_label = AsDiscrete(to_onehot=num_classes)              # eval specific\n",
    "        self.tta_val = tta_val\n",
    "        \n",
    "        orig_label_classes, target_label_classes = (\n",
    "            np.array([   0,    2,    3,    4,    5,    7,    8,   10,   11,   12,   13,\n",
    "                14,   15,   16,   17,   18,   24,   26,   28,   30,   31,   41,\n",
    "                42,   43,   44,   46,   47,   49,   50,   51,   52,   53,   54,\n",
    "                58,   60,   62,   63,   77,   80,   85,  251,  252,  253,  254,\n",
    "                255, 1000, 1002, 1003, 1005, 1006, 1007, 1008, 1009, 1010, 1011,\n",
    "            1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022,\n",
    "            1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1034, 1035,\n",
    "            2000, 2002, 2003, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012,\n",
    "            2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023,\n",
    "            2024, 2025, 2026, 2027, 2028, 2029, 2030, 2031, 2034, 2035], dtype=np.float64),\n",
    "            np.array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
    "                13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
    "                26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
    "                39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
    "                52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
    "                65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
    "                78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
    "                91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
    "            104, 105, 106, 107, 108], dtype=np.float64)\n",
    "        )\n",
    "        self.orig_label_classes = orig_label_classes\n",
    "        self.target_label_classes = target_label_classes\n",
    "        # self.post_trans = Compose([\n",
    "        #     AddChannel(),\n",
    "        #     MapLabelValue(    # 0~108로 잡혀있는 label을 original index로 변경\n",
    "        #                   orig_labels=self.target_label_classes, \n",
    "        #                   target_labels=self.orig_label_classes,\n",
    "        #                   dtype=np.uint8\n",
    "        #                   )\n",
    "        #     ])\n",
    "        self.post_trans = MapLabelValue(    # 0~108로 잡혀있는 label을 original index로 변경\n",
    "                                        orig_labels=self.target_label_classes, \n",
    "                                        target_labels=self.orig_label_classes\n",
    "                          )\n",
    "\n",
    "    def _iteration(\n",
    "        self, engine: Engine, batchdata: Dict[str, Any]\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        callback function for the Supervised Evaluation processing logic of 1 iteration in Ignite Engine.\n",
    "        Return below items in a dictionary:\n",
    "            - IMAGE: image Tensor data for model input, already moved to device.\n",
    "            - LABEL: label Tensor data corresponding to the image, already moved to device.\n",
    "            - PRED: prediction result of model.\n",
    "\n",
    "        Args:\n",
    "            engine: Ignite Engine, it can be a trainer, validator or evaluator.\n",
    "            batchdata: input data for this iteration, usually can be dictionary or tuple of Tensor data.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: When ``batchdata`` is None.\n",
    "\n",
    "        \"\"\"\n",
    "        if batchdata is None:\n",
    "            raise ValueError(\"Must provide batch data for current iteration.\")\n",
    "        batch = self.prepare_batch(batchdata, engine.state.device, engine.non_blocking)\n",
    "        if len(batch) == 2:\n",
    "            inputs, targets = batch\n",
    "            args: Tuple = ()\n",
    "            kwargs: Dict = {}\n",
    "        else:\n",
    "            inputs, targets, args, kwargs = batch\n",
    "        \n",
    "        start = time.time()\n",
    "\n",
    "        targets = targets.cpu()      # device로 바꿔보자\n",
    "        # print('CPU 비활성화!!!!!!!!!! ')\n",
    "\n",
    "        def _compute_pred():\n",
    "            ct = 1.0\n",
    "            # pred = self.inferer(inputs, self.network, *args, **kwargs).cpu()    # device로 바꿔보자 (지우면댐)\n",
    "            pred = self.inferer(inputs, self.network, *args, **kwargs)    # device로 바꿔보자 (지우면댐)\n",
    "            pred = nn.functional.softmax(pred, dim=1)\n",
    "            if not self.tta_val:\n",
    "                return pred\n",
    "            else:\n",
    "                for dims in [[2], [3], [4], (2, 3), (2, 4), (3, 4), (2, 3, 4)]:\n",
    "                    flip_inputs = torch.flip(inputs, dims=dims)\n",
    "                    flip_pred = torch.flip(\n",
    "                        # self.inferer(flip_inputs, self.network).cpu(), dims=dims    # device로 바꿔보자\n",
    "                        self.inferer(flip_inputs, self.network), dims=dims    # device로 바꿔보자\n",
    "                    )\n",
    "                    flip_pred = nn.functional.softmax(flip_pred, dim=1)\n",
    "                    del flip_inputs\n",
    "                    pred += flip_pred\n",
    "                    del flip_pred\n",
    "                    ct += 1\n",
    "                return pred / ct\n",
    "\n",
    "        # execute forward computation\n",
    "        with eval_mode(self.network):\n",
    "            if self.amp:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    predictions = _compute_pred()\n",
    "            else:\n",
    "                predictions = _compute_pred()\n",
    "\n",
    "        inputs = inputs.cpu()    #  # device로 바꿔보자\n",
    "        print(\"inference & GPU to CPU time :\", time.time() - start)\n",
    "\n",
    "        start = time.time()\n",
    "        predictions = self.post_pred(decollate_batch(predictions)[0])\n",
    "        targets = self.post_label(decollate_batch(targets)[0])                # eval specific\n",
    "\n",
    "        affine = batchdata[\"image_meta_dict\"][\"affine\"].numpy()[0]            # infer specific\n",
    "        resample_flag = batchdata[\"resample_flag\"]\n",
    "        anisotrophy_flag = batchdata[\"anisotrophy_flag\"]\n",
    "        crop_shape = batchdata[\"crop_shape\"][0].tolist()\n",
    "        original_shape = batchdata[\"original_shape\"][0].tolist()\n",
    "        if resample_flag:\n",
    "            # convert the prediction back to the original (after cropped) shape\n",
    "            predictions = recovery_prediction(\n",
    "                predictions.numpy(), [self.num_classes, *crop_shape], anisotrophy_flag\n",
    "            )\n",
    "            predictions = torch.tensor(predictions)\n",
    "        print(\"post process time :\", time.time() - start)\n",
    "\n",
    "        start = time.time()\n",
    "        ## 이미지 저장\n",
    "        predictions_wirte = predictions.cpu()\n",
    "#         print(type(predictions_wirte))\n",
    "#         print(predictions_wirte.shape)\n",
    "        predictions_wirte = np.argmax(predictions_wirte, axis=0)\n",
    "#         print(predictions_wirte.shape)\n",
    "        predictions_wirte_org = np.zeros([*original_shape])\n",
    "        \n",
    "        # put iteration outputs into engine.state\n",
    "        engine.state.output = {Keys.IMAGE: inputs, Keys.LABEL: targets.unsqueeze(0)}\n",
    "        engine.state.output[Keys.PRED] = torch.zeros([1, self.num_classes, *original_shape])\n",
    "        # pad the prediction back to the original shape\n",
    "        box_start, box_end = batchdata[\"bbox\"][0]\n",
    "        h_start, w_start, d_start = box_start\n",
    "        h_end, w_end, d_end = box_end\n",
    "\n",
    "        engine.state.output[Keys.PRED][\n",
    "            0, :, h_start:h_end, w_start:w_end, d_start:d_end\n",
    "        ] = predictions\n",
    "        del predictions\n",
    "\n",
    "        \n",
    "       \n",
    "        predictions_wirte_org[h_start:h_end, w_start:w_end, d_start:d_end] = predictions_wirte\n",
    "        del predictions_wirte\n",
    "#         print('변형전 dtype', predictions_wirte_org.dtype)\n",
    "#         print(np.unique(predictions_wirte_org))\n",
    "        predictions_wirte_org = self.post_trans(predictions_wirte_org)   # 원래대로 pred index 복구\n",
    "#         print('변형후 dtype', predictions_wirte_org.dtype)\n",
    "#         print(np.unique(predictions_wirte_org))\n",
    "        \n",
    "        filename = batchdata[\"image_meta_dict\"][\"filename_or_obj\"][0].split(\"/\")[-1]\n",
    "        print(\n",
    "            \"save {} with shape: {}\".format(\n",
    "                filename, predictions_wirte_org.shape\n",
    "            )\n",
    "        )\n",
    "#         write_nifti(\n",
    "#             data=predictions_wirte_org,\n",
    "#             file_name=os.path.join(self.output_dir, filename),\n",
    "#             affine=affine,\n",
    "#             resample=False,\n",
    "#             output_dtype=np.uint32,\n",
    "#         )\n",
    "#         print(f'MRI img:{ filename } eval done .................')\n",
    "        print(\"save img time :\", time.time() - start)\n",
    "    \n",
    "        start = time.time()\n",
    "        engine.fire_event(IterationEvents.FORWARD_COMPLETED)\n",
    "        engine.fire_event(IterationEvents.MODEL_COMPLETED)\n",
    "        print(\"fire event time :\", time.time() - start)\n",
    "\n",
    "        return engine.state.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6d955515",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 6. GPU inference 테스트 - 어디까지 GPU가능한건지 확인\n",
    "net.eval()\n",
    "evaluator = DynUNetEvaluator_GPU_test(\n",
    "    device=device,\n",
    "    val_data_loader=val_loader,\n",
    "    network=net,\n",
    "    output_dir=val_output_dir,\n",
    "    num_classes=num_classes,\n",
    "    inferer=SlidingWindowInferer(\n",
    "        roi_size=patch_size,\n",
    "        sw_batch_size=val_batch_size,\n",
    "        overlap=eval_overlap,\n",
    "        mode=window_mode,\n",
    "    ),\n",
    "    postprocessing=None,\n",
    "    key_val_metric={\n",
    "        \"val_mean_dice\": MeanDice(\n",
    "            include_background=False,\n",
    "            output_transform=from_engine([\"pred\", \"label\"]),\n",
    "        )\n",
    "    },\n",
    "    additional_metrics=None,\n",
    "    amp=amp,\n",
    "    tta_val=tta_val,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "fd8160c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynUNetEvaluator(SupervisedEvaluator):\n",
    "    \"\"\"\n",
    "    This class inherits from SupervisedEvaluator in MONAI, and is used with DynUNet\n",
    "    on Decathlon datasets.\n",
    "\n",
    "    Args:\n",
    "        device: an object representing the device on which to run.\n",
    "        val_data_loader: Ignite engine use data_loader to run, must be\n",
    "            torch.DataLoader.\n",
    "        network: use the network to run model forward.\n",
    "        num_classes: the number of classes (output channels) for the task.\n",
    "        epoch_length: number of iterations for one epoch, default to\n",
    "            `len(val_data_loader)`.\n",
    "        non_blocking: if True and this copy is between CPU and GPU, the copy may occur asynchronously\n",
    "            with respect to the host. For other cases, this argument has no effect.\n",
    "        prepare_batch: function to parse image and label for current iteration.\n",
    "        iteration_update: the callable function for every iteration, expect to accept `engine`\n",
    "            and `batchdata` as input parameters. if not provided, use `self._iteration()` instead.\n",
    "        inferer: inference method that execute model forward on input data, like: SlidingWindow, etc.\n",
    "        postprocessing: execute additional transformation for the model output data.\n",
    "            Typically, several Tensor based transforms composed by `Compose`.\n",
    "        key_val_metric: compute metric when every iteration completed, and save average value to\n",
    "            engine.state.metrics when epoch completed. key_val_metric is the main metric to compare and save the\n",
    "            checkpoint into files.\n",
    "        additional_metrics: more Ignite metrics that also attach to Ignite Engine.\n",
    "        val_handlers: every handler is a set of Ignite Event-Handlers, must have `attach` function, like:\n",
    "            CheckpointHandler, StatsHandler, SegmentationSaver, etc.\n",
    "        amp: whether to enable auto-mixed-precision evaluation, default is False.\n",
    "        tta_val: whether to do the 8 flips (8 = 2 ** 3, where 3 represents the three dimensions)\n",
    "            test time augmentation, default is False.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        device: torch.device,\n",
    "        val_data_loader: DataLoader,\n",
    "        network: torch.nn.Module,\n",
    "        num_classes: Union[str, int],\n",
    "        epoch_length: Optional[int] = None,\n",
    "        non_blocking: bool = False,\n",
    "        prepare_batch: Callable = default_prepare_batch,\n",
    "        iteration_update: Optional[Callable] = None,\n",
    "        inferer: Optional[Inferer] = None,\n",
    "        postprocessing: Optional[Transform] = None,\n",
    "        key_val_metric: Optional[Dict[str, Metric]] = None,\n",
    "        additional_metrics: Optional[Dict[str, Metric]] = None,\n",
    "        val_handlers: Optional[Sequence] = None,\n",
    "        amp: bool = False,\n",
    "        tta_val: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__(\n",
    "            device=device,\n",
    "            val_data_loader=val_data_loader,\n",
    "            network=network,\n",
    "            epoch_length=epoch_length,\n",
    "            non_blocking=non_blocking,\n",
    "            prepare_batch=prepare_batch,\n",
    "            iteration_update=iteration_update,\n",
    "            inferer=inferer,\n",
    "            postprocessing=postprocessing,\n",
    "            key_val_metric=key_val_metric,\n",
    "            additional_metrics=additional_metrics,\n",
    "            val_handlers=val_handlers,\n",
    "            amp=amp,\n",
    "        )\n",
    "\n",
    "        if not isinstance(num_classes, int):\n",
    "            num_classes = int(num_classes)\n",
    "        self.num_classes = num_classes\n",
    "        self.post_pred = AsDiscrete(argmax=True, to_onehot=num_classes)\n",
    "        self.post_label = AsDiscrete(to_onehot=num_classes)              # eval specific\n",
    "        self.tta_val = tta_val\n",
    "\n",
    "    def _iteration(\n",
    "        self, engine: Engine, batchdata: Dict[str, Any]\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        callback function for the Supervised Evaluation processing logic of 1 iteration in Ignite Engine.\n",
    "        Return below items in a dictionary:\n",
    "            - IMAGE: image Tensor data for model input, already moved to device.\n",
    "            - LABEL: label Tensor data corresponding to the image, already moved to device.\n",
    "            - PRED: prediction result of model.\n",
    "\n",
    "        Args:\n",
    "            engine: Ignite Engine, it can be a trainer, validator or evaluator.\n",
    "            batchdata: input data for this iteration, usually can be dictionary or tuple of Tensor data.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: When ``batchdata`` is None.\n",
    "\n",
    "        \"\"\"\n",
    "        start = time.time()\n",
    "        \n",
    "        if batchdata is None:\n",
    "            raise ValueError(\"Must provide batch data for current iteration.\")\n",
    "        batch = self.prepare_batch(batchdata, engine.state.device, engine.non_blocking)\n",
    "        if len(batch) == 2:\n",
    "            inputs, targets = batch\n",
    "            args: Tuple = ()\n",
    "            kwargs: Dict = {}\n",
    "        else:\n",
    "            inputs, targets, args, kwargs = batch\n",
    "\n",
    "#         targets = targets.cpu()      # device로 바꿔보자\n",
    "        print(\"batch 나누고 target cpu time :\", time.time() - start)\n",
    "\n",
    "        start = time.time()\n",
    "        def _compute_pred():\n",
    "            ct = 1.0\n",
    "#             pred = self.inferer(inputs, self.network, *args, **kwargs).cpu()    # device로 바꿔보자 (지우면댐)\n",
    "            pred = self.inferer(inputs, self.network, *args, **kwargs)    # device로 바꿔보자 (지우면댐)\n",
    "            pred = nn.functional.softmax(pred, dim=1)\n",
    "            if not self.tta_val:\n",
    "                return pred\n",
    "            else:\n",
    "                for dims in [[2], [3], [4], (2, 3), (2, 4), (3, 4), (2, 3, 4)]:\n",
    "                    flip_inputs = torch.flip(inputs, dims=dims)\n",
    "                    flip_pred = torch.flip(\n",
    "                        self.inferer(flip_inputs, self.network).cpu(), dims=dims    # device로 바꿔보자\n",
    "                    )\n",
    "                    flip_pred = nn.functional.softmax(flip_pred, dim=1)\n",
    "                    del flip_inputs\n",
    "                    pred += flip_pred\n",
    "                    del flip_pred\n",
    "                    ct += 1\n",
    "                return pred / ct\n",
    "\n",
    "        # execute forward computation\n",
    "        with eval_mode(self.network):\n",
    "            if self.amp:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    predictions = _compute_pred()\n",
    "            else:\n",
    "                predictions = _compute_pred()\n",
    "\n",
    "        inputs = inputs.cpu()    #  # device로 바꿔보자\n",
    "        print(\"inference & input cpu time :\", time.time() - start)\n",
    "\n",
    "        start = time.time()\n",
    "        predictions = self.post_pred(decollate_batch(predictions)[0])\n",
    "        targets = self.post_label(decollate_batch(targets)[0])                # eval specific\n",
    "        print(\"post transform time :\", time.time() - start)\n",
    "\n",
    "        start = time.time()\n",
    "        resample_flag = batchdata[\"resample_flag\"]\n",
    "        anisotrophy_flag = batchdata[\"anisotrophy_flag\"]\n",
    "        crop_shape = batchdata[\"crop_shape\"][0].tolist()\n",
    "        original_shape = batchdata[\"original_shape\"][0].tolist()\n",
    "        if resample_flag:\n",
    "            # convert the prediction back to the original (after cropped) shape\n",
    "            predictions = recovery_prediction(\n",
    "                predictions.numpy(), [self.num_classes, *crop_shape], anisotrophy_flag\n",
    "            )\n",
    "            predictions = torch.tensor(predictions)\n",
    "        print(\"post process time :\", time.time() - start)\n",
    "\n",
    "        start = time.time()\n",
    "        targets = targets.cpu()\n",
    "        # put iteration outputs into engine.state\n",
    "        engine.state.output = {Keys.IMAGE: inputs, Keys.LABEL: targets.unsqueeze(0)}\n",
    "        engine.state.output[Keys.PRED] = torch.zeros([1, self.num_classes, *original_shape])\n",
    "        print(\"engine state output IMAGE, LABEL time :\", time.time() - start)\n",
    "        \n",
    "        start = time.time()\n",
    "        # pad the prediction back to the original shape\n",
    "        box_start, box_end = batchdata[\"bbox\"][0]\n",
    "        h_start, w_start, d_start = box_start\n",
    "        h_end, w_end, d_end = box_end\n",
    "        print(\"padding step1 :\", time.time() - start)\n",
    "\n",
    "        \n",
    "        start = time.time()\n",
    "        engine.state.output[Keys.PRED][\n",
    "            0, :, h_start:h_end, w_start:w_end, d_start:d_end\n",
    "        ] = predictions\n",
    "        del predictions\n",
    "        print(\"engine state output PRED time :\", time.time() - start)\n",
    "\n",
    "        \n",
    "        \n",
    "        # filename = batchdata[\"image_meta_dict\"][\"filename_or_obj\"][0].split(\"/\")[-1]\n",
    "        # print(f'MRI img:{ filename } eval done .................')\n",
    "        \n",
    "        engine.fire_event(IterationEvents.FORWARD_COMPLETED)\n",
    "        engine.fire_event(IterationEvents.MODEL_COMPLETED)\n",
    "\n",
    "        return engine.state.output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "fc76e0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 6. GPU inference 테스트 - 어디까지 GPU가능한건지 확인\n",
    "net.eval()\n",
    "evaluator = DynUNetEvaluator(\n",
    "    device=device,\n",
    "    val_data_loader=val_loader,\n",
    "    network=net,\n",
    "#     output_dir=val_output_dir,\n",
    "    num_classes=num_classes,\n",
    "    inferer=SlidingWindowInferer(\n",
    "        roi_size=patch_size,\n",
    "        sw_batch_size=val_batch_size,\n",
    "        overlap=eval_overlap,\n",
    "        mode=window_mode,\n",
    "    ),\n",
    "    postprocessing=None,\n",
    "    key_val_metric={\n",
    "        \"val_mean_dice\": MeanDice(\n",
    "            include_background=False,\n",
    "            output_transform=from_engine([\"pred\", \"label\"]),\n",
    "        )\n",
    "    },\n",
    "    additional_metrics=None,\n",
    "    amp=amp,\n",
    "    tta_val=tta_val,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "de94bedb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 나누고 target cpu time : 0.021812915802001953\n",
      "inference & input cpu time : 0.9027938842773438\n",
      "post transform time : 0.0031576156616210938\n",
      "post process time : 0.0003266334533691406\n",
      "engine state output IMAGE, LABEL time : 8.171430587768555\n",
      "padding step1 : 0.0018572807312011719\n",
      "engine state output PRED time : 2.464026689529419\n",
      "batch 나누고 target cpu time : 0.023492097854614258\n",
      "inference & input cpu time : 0.3713648319244385\n",
      "post transform time : 0.001184225082397461\n",
      "post process time : 0.00028824806213378906\n",
      "engine state output IMAGE, LABEL time : 2.120267629623413\n",
      "padding step1 : 0.0009603500366210938\n",
      "engine state output PRED time : 0.61195969581604\n",
      "time : 57.13516139984131\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "evaluator.run()\n",
    "print(\"time :\", time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31694800",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128c25aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8b284a1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 나누고 target cpu time : 0.010008573532104492\n",
      "inference & input cpu time : 0.6507704257965088\n",
      "post transform time : 0.0016832351684570312\n",
      "post process time : 0.0003173351287841797\n",
      "engine state output IMAGE, LABEL time : 9.368163585662842\n",
      "padding step1 : 0.0011758804321289062\n",
      "engine state output PRED time : 1.7084453105926514\n",
      "batch 나누고 target cpu time : 0.007800102233886719\n",
      "inference & input cpu time : 0.22352385520935059\n",
      "post transform time : 0.0010840892791748047\n",
      "post process time : 8.344650268554688e-05\n",
      "engine state output IMAGE, LABEL time : 2.3483664989471436\n",
      "padding step1 : 0.00038814544677734375\n",
      "engine state output PRED time : 0.6502490043640137\n",
      "time : 46.154680490493774\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "evaluator.run()\n",
    "print(\"time :\", time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "eae7e406",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 나누고 target cpu time : 0.00948786735534668\n",
      "inference & input cpu time : 0.6287620067596436\n",
      "post transform time : 0.0023174285888671875\n",
      "post process time : 0.0005431175231933594\n",
      "engine state output IMAGE, LABEL time : 5.265115022659302\n",
      "padding step1 : 0.00231170654296875\n",
      "engine state output PRED time : 2.101475238800049\n",
      "batch 나누고 target cpu time : 0.009245872497558594\n",
      "inference & input cpu time : 0.221177339553833\n",
      "post transform time : 0.00106048583984375\n",
      "post process time : 7.43865966796875e-05\n",
      "engine state output IMAGE, LABEL time : 2.5195584297180176\n",
      "padding step1 : 0.0003178119659423828\n",
      "engine state output PRED time : 0.6775071620941162\n",
      "time : 44.54240584373474\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "evaluator.run()\n",
    "print(\"time :\", time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ddea4c51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 나누고 target cpu time : 0.012660741806030273\n",
      "inference & input cpu time : 0.6355206966400146\n",
      "post transform time : 0.00115203857421875\n",
      "post process time : 9.846687316894531e-05\n",
      "engine state output IMAGE, LABEL time : 4.576287508010864\n",
      "padding step1 : 0.0018472671508789062\n",
      "engine state output PRED time : 2.8230185508728027\n",
      "batch 나누고 target cpu time : 0.009508371353149414\n",
      "inference & input cpu time : 0.2380521297454834\n",
      "post transform time : 0.0009570121765136719\n",
      "post process time : 5.91278076171875e-05\n",
      "engine state output IMAGE, LABEL time : 2.501232147216797\n",
      "padding step1 : 0.0008628368377685547\n",
      "engine state output PRED time : 0.7110633850097656\n",
      "time : 50.182448387145996\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "evaluator.run()\n",
    "print(\"time :\", time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "dc03147e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 나누고 target cpu time : 0.008741378784179688\n",
      "inference & input cpu time : 0.643047571182251\n",
      "post transform time : 0.0014870166778564453\n",
      "post process time : 0.00011467933654785156\n",
      "engine state output IMAGE, LABEL time : 7.032412052154541\n",
      "padding step1 : 0.002337932586669922\n",
      "engine state output PRED time : 1.9719617366790771\n",
      "batch 나누고 target cpu time : 0.008804559707641602\n",
      "inference & input cpu time : 0.22884535789489746\n",
      "post transform time : 0.0010373592376708984\n",
      "post process time : 0.0003533363342285156\n",
      "engine state output IMAGE, LABEL time : 2.4040772914886475\n",
      "padding step1 : 0.0015950202941894531\n",
      "engine state output PRED time : 0.8416135311126709\n",
      "time : 46.7656512260437\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "evaluator.run()\n",
    "print(\"time :\", time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5d96b30b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 나누고 target cpu time : 0.010185956954956055\n",
      "inference & input cpu time : 0.6356122493743896\n",
      "post transform time : 0.001585245132446289\n",
      "post process time : 0.00017881393432617188\n",
      "engine state output IMAGE, LABEL time : 33.64306569099426\n",
      "padding step1 : 0.0016582012176513672\n",
      "engine state output PRED time : 22.492120027542114\n",
      "batch 나누고 target cpu time : 0.006925821304321289\n",
      "inference & input cpu time : 0.22810983657836914\n",
      "post transform time : 0.0013251304626464844\n",
      "post process time : 0.0004665851593017578\n",
      "engine state output IMAGE, LABEL time : 2.1834659576416016\n",
      "padding step1 : 0.0003027915954589844\n",
      "engine state output PRED time : 0.7112226486206055\n",
      "time : 173.87773871421814\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "evaluator.run()\n",
    "print(\"time :\", time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7fd8261d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 나누고 target cpu time : 0.01003265380859375\n",
      "inference & input cpu time : 0.6416950225830078\n",
      "post transform time : 0.013015031814575195\n",
      "post process time : 0.0005283355712890625\n",
      "engine state output IMAGE, LABEL time : 4.64120078086853\n",
      "padding step1 : 0.0021772384643554688\n",
      "engine state output PRED time : 1.2226996421813965\n",
      "batch 나누고 target cpu time : 0.007966995239257812\n",
      "inference & input cpu time : 0.24000000953674316\n",
      "post transform time : 0.0009665489196777344\n",
      "post process time : 6.771087646484375e-05\n",
      "engine state output IMAGE, LABEL time : 2.509558916091919\n",
      "padding step1 : 0.0010721683502197266\n",
      "engine state output PRED time : 0.8421065807342529\n",
      "time : 98.78075790405273\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "evaluator.run()\n",
    "print(\"time :\", time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecd7542",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "401f8e17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 나누고 target cpu time : 0.13880324363708496\n",
      "inference & input cpu time : 0.7224211692810059\n",
      "post transform time : 8.644158363342285\n",
      "post process time : 0.008751869201660156\n",
      "engine state output IMAGE, LABEL time : 9.609595537185669\n",
      "padding step1 : 0.002599000930786133\n",
      "engine state output PRED time : 5.351591110229492\n",
      "batch 나누고 target cpu time : 0.4457237720489502\n",
      "inference & input cpu time : 0.33050537109375\n",
      "post transform time : 0.9618697166442871\n",
      "post process time : 0.0004565715789794922\n",
      "engine state output IMAGE, LABEL time : 0.4404935836791992\n",
      "padding step1 : 0.000682830810546875\n",
      "engine state output PRED time : 0.7321929931640625\n",
      "time : 313.5694110393524\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "evaluator.run()\n",
    "print(\"time :\", time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6a57876e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 나누고 target cpu time : 0.03365278244018555\n",
      "inference & input cpu time : 0.6299469470977783\n",
      "post transform time : 1.790766716003418\n",
      "post process time : 0.008248329162597656\n",
      "engine state output IMAGE, LABEL time : 0.26795172691345215\n",
      "padding step1 : 0.02272510528564453\n",
      "engine state output PRED time : 0.9441828727722168\n",
      "batch 나누고 target cpu time : 0.016907691955566406\n",
      "inference & input cpu time : 0.22605657577514648\n",
      "post transform time : 0.29419970512390137\n",
      "post process time : 0.00028514862060546875\n",
      "engine state output IMAGE, LABEL time : 0.2901937961578369\n",
      "padding step1 : 0.009162664413452148\n",
      "engine state output PRED time : 0.8744888305664062\n",
      "time : 36.87901425361633\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "evaluator.run()\n",
    "print(\"time :\", time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982bbad2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680962c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "52b03835",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 나누고 target cpu time : 0.02635049819946289\n",
      "inference & input cpu time : 2.158511161804199\n",
      "post transform time : 1.7476403713226318\n",
      "post process time : 0.007225751876831055\n",
      "engine state output IMAGE, LABEL time : 0.902824878692627\n",
      "padding step1 : 0.0026252269744873047\n",
      "engine state output PRED time : 0.24624967575073242\n",
      "batch 나누고 target cpu time : 0.013247489929199219\n",
      "inference & input cpu time : 1.3362798690795898\n",
      "post transform time : 1.1730046272277832\n",
      "post process time : 0.0009338855743408203\n",
      "engine state output IMAGE, LABEL time : 0.4753870964050293\n",
      "padding step1 : 0.00940847396850586\n",
      "engine state output PRED time : 0.2286827564239502\n",
      "time : 27.524839639663696\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "evaluator.run()\n",
    "print(\"time :\", time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7afdf2ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 나누고 target cpu time : 0.03188371658325195\n",
      "inference & input cpu time : 2.4179773330688477\n",
      "post transform time : 3.0485117435455322\n",
      "post process time : 0.005202770233154297\n",
      "engine state output IMAGE, LABEL time : 1.761392593383789\n",
      "padding step1 : 0.0015418529510498047\n",
      "engine state output PRED time : 0.2728283405303955\n",
      "batch 나누고 target cpu time : 0.014521121978759766\n",
      "inference & input cpu time : 1.3466269969940186\n",
      "post transform time : 1.2124178409576416\n",
      "post process time : 0.0007166862487792969\n",
      "engine state output IMAGE, LABEL time : 0.37946271896362305\n",
      "padding step1 : 0.0004837512969970703\n",
      "engine state output PRED time : 0.2589864730834961\n",
      "time : 30.914069652557373\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "evaluator.run()\n",
    "print(\"time :\", time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dcf245a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 나누고 target cpu time : 0.028201580047607422\n",
      "inference & input cpu time : 2.6068193912506104\n",
      "post transform time : 1.3740296363830566\n",
      "post process time : 0.006607532501220703\n",
      "engine state output IMAGE, LABEL time : 1.0256309509277344\n",
      "padding step1 : 0.0025115013122558594\n",
      "engine state output PRED time : 0.22584128379821777\n",
      "batch 나누고 target cpu time : 0.014957666397094727\n",
      "inference & input cpu time : 1.3947994709014893\n",
      "post transform time : 1.2087392807006836\n",
      "post process time : 0.0009834766387939453\n",
      "engine state output IMAGE, LABEL time : 0.3890504837036133\n",
      "padding step1 : 0.0004909038543701172\n",
      "engine state output PRED time : 0.24766159057617188\n",
      "time : 32.28419256210327\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "evaluator.run()\n",
    "print(\"time :\", time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "62379fbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 나누고 target cpu time : 0.029281139373779297\n",
      "inference & input cpu time : 2.3431434631347656\n",
      "post transform time : 1.6407890319824219\n",
      "post process time : 0.001245737075805664\n",
      "engine state output IMAGE, LABEL time : 1.2049484252929688\n",
      "padding step1 : 0.002322673797607422\n",
      "engine state output PRED time : 0.31487178802490234\n",
      "batch 나누고 target cpu time : 0.02144336700439453\n",
      "inference & input cpu time : 1.507706880569458\n",
      "post transform time : 1.1656773090362549\n",
      "post process time : 0.0008268356323242188\n",
      "engine state output IMAGE, LABEL time : 0.39177393913269043\n",
      "padding step1 : 0.00037217140197753906\n",
      "engine state output PRED time : 0.2459874153137207\n",
      "time : 57.72733664512634\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "evaluator.run()\n",
    "print(\"time :\", time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4c09b456",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 나누고 target cpu time : 0.04864764213562012\n",
      "inference & input cpu time : 5.725265026092529\n",
      "post transform time : 36.36089324951172\n",
      "post process time : 0.022902965545654297\n",
      "engine state output IMAGE, LABEL time : 38.6579794883728\n",
      "padding step1 : 0.001954317092895508\n",
      "engine state output PRED time : 0.6220638751983643\n",
      "batch 나누고 target cpu time : 0.023790597915649414\n",
      "inference & input cpu time : 1.4547474384307861\n",
      "post transform time : 1.4021012783050537\n",
      "post process time : 0.0006260871887207031\n",
      "engine state output IMAGE, LABEL time : 0.385875940322876\n",
      "padding step1 : 0.0010504722595214844\n",
      "engine state output PRED time : 0.27706384658813477\n",
      "time : 124.70445346832275\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "evaluator.run()\n",
    "print(\"time :\", time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94dcf2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ffd9e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ec0ccf94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference & GPU to CPU time : 0.833308219909668\n",
      "post process time : 20.509447813034058\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([109, 154, 154, 152])\n",
      "torch.Size([154, 154, 152])\n",
      "save s_SU0303_00_0_b.nii.gz with shape: (186, 230, 230)\n",
      "save img time : 72.48131132125854\n",
      "fire event time : 0.006493330001831055\n",
      "inference & GPU to CPU time : 0.3933119773864746\n",
      "post process time : 0.47392749786376953\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([109, 141, 146, 133])\n",
      "torch.Size([141, 146, 133])\n",
      "save s_SU0197_00_0_b.nii.gz with shape: (186, 230, 230)\n",
      "save img time : 4.3263585567474365\n",
      "fire event time : 0.006649017333984375\n",
      "time : 311.10440850257874\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "evaluator.run()\n",
    "print(\"time :\", time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e6bc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "a[93]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "401e7457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "torch.Size([1, 2, 154, 154, 152])\n",
      "1\n",
      "torch.Size([1, 2, 141, 146, 133])\n"
     ]
    }
   ],
   "source": [
    "for idx, i in enumerate(val_loader):\n",
    "    print(idx)\n",
    "    print(i['image'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1503f32c",
   "metadata": {},
   "source": [
    "이미지를 로드하여 mapping transform 테스트를 진행해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1647af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.transforms import (\n",
    "    LoadImaged,\n",
    "    AddChanneld,\n",
    "    MapLabelValued,\n",
    "    Compose\n",
    ")\n",
    "from monai.data import (\n",
    "    CacheDataset,\n",
    "    DataLoader,\n",
    "    partition_dataset,\n",
    ")\n",
    "\n",
    "orig_label_classes, target_label_classes = (\n",
    "    np.array([   0,    2,    3,    4,    5,    7,    8,   10,   11,   12,   13,\n",
    "         14,   15,   16,   17,   18,   24,   26,   28,   30,   31,   41,\n",
    "         42,   43,   44,   46,   47,   49,   50,   51,   52,   53,   54,\n",
    "         58,   60,   62,   63,   77,   80,   85,  251,  252,  253,  254,\n",
    "        255, 1000, 1002, 1003, 1005, 1006, 1007, 1008, 1009, 1010, 1011,\n",
    "       1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022,\n",
    "       1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1034, 1035,\n",
    "       2000, 2002, 2003, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012,\n",
    "       2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023,\n",
    "       2024, 2025, 2026, 2027, 2028, 2029, 2030, 2031, 2034, 2035]),\n",
    "    np.array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
    "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
    "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
    "        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
    "        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
    "        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
    "        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
    "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
    "       104, 105, 106, 107, 108])\n",
    ")\n",
    "\n",
    "def get_test_transform():\n",
    "    keys = [\"label\", \"pred\"]\n",
    "    transforms = [\n",
    "        LoadImaged(keys=keys),\n",
    "#         AddChanneld(keys=keys),\n",
    "#         MapLabelValued(\n",
    "#             keys=[\"pred\"], \n",
    "#             orig_labels=target_label_classes, \n",
    "#             target_labels=orig_label_classes\n",
    "#         ),\n",
    "#         Orientationd(keys=keys, axcodes=\"RAS\"),\n",
    "#         ConcatItemsd(keys=[\"image\", \"mask\"], name=\"image\"),\n",
    "#         PreprocessAnisotropic(\n",
    "#             keys=[\"image\"],\n",
    "#             clip_values=clip_values,\n",
    "#             pixdim=spacing,\n",
    "#             normalize_values=normalize_values,\n",
    "#             model_mode=\"test\",\n",
    "#         ),\n",
    "#         CastToTyped(keys=[\"image\"], dtype=(np.float32)),\n",
    "#         EnsureTyped(keys=[\"image\"]),\n",
    "    ]\n",
    "    return Compose(transforms)\n",
    "transform = get_test_transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a6434a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "# original_file = os.path.join(data_dir, \"s_SU0197_00_0_b.nii.gz\")\n",
    "# shutil.copy(original_file, './')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7b6a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "label_file = os.path.join(data_dir, \"s_SU0197_00_0_l.nii.gz\")\n",
    "pred_file = \"runs_eval2/s_SU0197_00_0_b.nii.gz\"\n",
    "data.append({'label': label_file, 'pred': pred_file})\n",
    "\n",
    "dataset = CacheDataset(\n",
    "    data=data,\n",
    "    transform=transform,\n",
    "    num_workers=8,\n",
    "    cache_rate=1.0,\n",
    ")\n",
    "\n",
    "data_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    drop_last=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698a5d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = first(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4327d813",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2484c374",
   "metadata": {},
   "outputs": [],
   "source": [
    "affine = test_data[\"pred_meta_dict\"][\"affine\"].numpy()[0]\n",
    "affine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50195c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['label'].shape, test_data['pred'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec60fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['pred'].dtype, test_data['label'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb82558",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(np.unique(test_data['pred'])), np.unique(test_data['pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc47810",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(test_data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3623580e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = np.where(test_data['pred'] == 2018)   # 2018, 93\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f256e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 12\n",
    "test_data['pred'][result[0][idx], result[1][idx], result[2][idx], result[3][idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55ab9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['label'][result[0][idx], result[1][idx], result[2][idx], result[3][idx]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb520ed",
   "metadata": {},
   "source": [
    "오케 맵핑이 잘되고 있음을 확인하였음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f171626a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e9b1f22c",
   "metadata": {},
   "source": [
    "------------------\n",
    "저장된 이미지로부터 post-transform mapping 하여 결과 확인해본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfd4653",
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.transforms import MapLabelValue\n",
    "orig_label_classes, target_label_classes = (\n",
    "    np.array([   0,    2,    3,    4,    5,    7,    8,   10,   11,   12,   13,\n",
    "        14,   15,   16,   17,   18,   24,   26,   28,   30,   31,   41,\n",
    "        42,   43,   44,   46,   47,   49,   50,   51,   52,   53,   54,\n",
    "        58,   60,   62,   63,   77,   80,   85,  251,  252,  253,  254,\n",
    "        255, 1000, 1002, 1003, 1005, 1006, 1007, 1008, 1009, 1010, 1011,\n",
    "    1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022,\n",
    "    1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1034, 1035,\n",
    "    2000, 2002, 2003, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012,\n",
    "    2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023,\n",
    "    2024, 2025, 2026, 2027, 2028, 2029, 2030, 2031, 2034, 2035]),\n",
    "    np.array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
    "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
    "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
    "        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
    "        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
    "        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
    "        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
    "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
    "    104, 105, 106, 107, 108])\n",
    ")\n",
    "post_trans = MapLabelValue(    # 0~108로 잡혀있는 label을 original index로 변경\n",
    "    orig_labels=target_label_classes, \n",
    "    target_labels=orig_label_classes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f15843f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['pred'].shape, test_data['label'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ce2246",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "aaa = post_trans(test_data['pred'])\n",
    "aaa.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b537b087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aaa = test_data['pred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d499240",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = np.where(aaa == 2018)   # 41\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ff527a",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 226\n",
    "aaa[result[0][idx], result[1][idx], result[2][idx], result[3][idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f0a114",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['label'][result[0][idx], result[1][idx], result[2][idx], result[3][idx]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e846231e",
   "metadata": {},
   "source": [
    "post transform 한 결과를 이미지로 저장한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040761a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "aaa.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2135698b",
   "metadata": {},
   "outputs": [],
   "source": [
    "aaa.squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b5051c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.data.nifti_writer import write_nifti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf26d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_nifti(\n",
    "        data=aaa.squeeze(),\n",
    "        file_name='aa2.nii.gz',\n",
    "        affine=affine,\n",
    "        resample=False,\n",
    "        output_dtype=np.uint8,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8eca068",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614e963b",
   "metadata": {},
   "outputs": [],
   "source": [
    "aaa.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd15e82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94ed36e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350a9fef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8417bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.transforms import (\n",
    "    LoadImaged,\n",
    "    AddChanneld,\n",
    "    MapLabelValued,\n",
    "    Compose\n",
    ")\n",
    "from monai.data import (\n",
    "    CacheDataset,\n",
    "    DataLoader,\n",
    "    partition_dataset,\n",
    ")\n",
    "\n",
    "orig_label_classes, target_label_classes = (\n",
    "    np.array([   0,    2,    3,    4,    5,    7,    8,   10,   11,   12,   13,\n",
    "         14,   15,   16,   17,   18,   24,   26,   28,   30,   31,   41,\n",
    "         42,   43,   44,   46,   47,   49,   50,   51,   52,   53,   54,\n",
    "         58,   60,   62,   63,   77,   80,   85,  251,  252,  253,  254,\n",
    "        255, 1000, 1002, 1003, 1005, 1006, 1007, 1008, 1009, 1010, 1011,\n",
    "       1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022,\n",
    "       1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1034, 1035,\n",
    "       2000, 2002, 2003, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012,\n",
    "       2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023,\n",
    "       2024, 2025, 2026, 2027, 2028, 2029, 2030, 2031, 2034, 2035]),\n",
    "    np.array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
    "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
    "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
    "        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
    "        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
    "        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
    "        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
    "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
    "       104, 105, 106, 107, 108])\n",
    ")\n",
    "\n",
    "def get_test_transform():\n",
    "    keys = [\"label\", \"pred\"]\n",
    "    transforms = [\n",
    "        LoadImaged(keys=keys),\n",
    "#         AddChanneld(keys=keys),\n",
    "#         MapLabelValued(\n",
    "#             keys=[\"pred\"], \n",
    "#             orig_labels=target_label_classes, \n",
    "#             target_labels=orig_label_classes\n",
    "#         ),\n",
    "#         Orientationd(keys=keys, axcodes=\"RAS\"),\n",
    "#         ConcatItemsd(keys=[\"image\", \"mask\"], name=\"image\"),\n",
    "#         PreprocessAnisotropic(\n",
    "#             keys=[\"image\"],\n",
    "#             clip_values=clip_values,\n",
    "#             pixdim=spacing,\n",
    "#             normalize_values=normalize_values,\n",
    "#             model_mode=\"test\",\n",
    "#         ),\n",
    "#         CastToTyped(keys=[\"image\"], dtype=(np.float32)),\n",
    "#         EnsureTyped(keys=[\"image\"]),\n",
    "    ]\n",
    "    return Compose(transforms)\n",
    "transform = get_test_transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932c7c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "label_file = os.path.join(data_dir, \"s_SU0197_00_0_l.nii.gz\")\n",
    "pred_file = \"aa2.nii.gz\"\n",
    "data.append({'label': label_file, 'pred': pred_file})\n",
    "\n",
    "dataset = CacheDataset(\n",
    "    data=data,\n",
    "    transform=transform,\n",
    "    num_workers=8,\n",
    "    cache_rate=1.0,\n",
    ")\n",
    "\n",
    "data_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    drop_last=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff375cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = first(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42259bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2257dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = np.where(test_data['pred'] == 2018)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b7ef13",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 12\n",
    "test_data['pred'][result[0][idx], result[1][idx], result[2][idx], result[3][idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04a126d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['label'][result[0][idx], result[1][idx], result[2][idx], result[3][idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3131f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d37d2731",
   "metadata": {},
   "source": [
    "## finding 정리\n",
    "evaluator에 MeanDice가 저장되는 개념.\n",
    " * evaluator.state.metrics : 전체 ROI MeanDice 평균\n",
    " * evaluator.state.metric_details[\"val_mean_dice\"] : ROI별 MeanDice 평균"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d3c2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if local_rank == 0:\n",
    "    print(evaluator.state.metrics)\n",
    "    results = evaluator.state.metric_details[\"val_mean_dice\"]\n",
    "    if num_classes > 2:\n",
    "        for i in range(num_classes - 1):\n",
    "            print(\n",
    "                \"mean dice for label {} is {}\".format(i + 1, results[:, i].mean())\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d491afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64228afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13caf267",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(evaluator.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370bea0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(evaluator.state.metric_details), evaluator.state.metric_details.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c84fdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.state.metric_details['val_mean_dice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4288d21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.state.metric_details['val_mean_dice'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336493ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1c774d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdcc675",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4157bd4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
